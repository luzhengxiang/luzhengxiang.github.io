<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql事务与锁]]></title>
    <url>%2F2020%2F03%2F26%2Fmysql%E4%BA%8B%E5%8A%A1%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[什么是数据库的事务事务的典型场景在项目里面，什么地方会开启事务，或者配置了事务?无论是在方法上加注解，还是配置切面。 12345678&lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="save*" rollback-for="Throwable" /&gt; &lt;tx:method name="add*" rollback-for="Throwable" /&gt; &lt;tx:method name="send*" rollback-for="Throwable" /&gt; &lt;tx:method name="insert*" rollback-for="Throwable" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 比如下单，会操作订单表，资金表，物流表等等，这个时候我们需要让这些操作都 在一个事务里面完成。当一个业务流程涉及多个表的操作的时候，我们希望它们要么是 全部成功的，要么都不成功，这个时候我们会启用事务。 在金融的系统里面事务配置是很常见的，比如行内转账的这种操作，如果我们把它简单地理解为一个账户的余额增加，另一个账户的余额减少的情况(当然实际上要比这复杂)，那么这两个动作一定是同时成功或者同时失败的，否则就会造成银行的会计科目不平衡。 事务的定义什么是事务维基百科的定义:事务是数据库管理系统(DBMS)执行过程中的一个逻辑单位，由 一个有限的数据库操作序列构成。这里面有两个关键点，第一个，它是数据库最小的工作单元，是不可以再分的。第 二个，它可能包含了一个或者一系列的 DML 语句，包括 insert delete update。(单条 DDL(create drop)和 DCL(grant revoke)也会有事务) 哪些存储引擎支持事务在我们第一天的课里面说到了，InnoDB 支持事务，这个也是它成为默认的存储引擎 的一个重要原因:https://dev.mysql.com/doc/refman/5.7/en/storage-engines.html另一个是 NDB。 事务的四大特性事务的四大特性:ACID。第一个，原子性，Atomicity，也就是我们刚才说的不可再分，也就意味着我们对数 据库的一系列的操作，要么都是成功，要么都是失败，不可能出现部分成功或者部分失 败的情况。以转账的场景为例，一个账户的余额减少，对应一个账户的增加，这两个一 定是同时成功或者同时失败的。全部成功比较简单，问题是如果前面一个操作已经成功了，后面的操作失败了，怎 么让它全部失败呢?这个时候我们必须要回滚。原子性，在 InnoDB 里面是通过 undo log 来实现的，它记录了数据修改之前的值(逻辑日志)，一旦发生异常，就可以用 undo log 来实现回滚操作。 第二个，一致性，consistent，指的是数据库的完整性约束没有被破坏，事务执行的 前后都是合法的数据状态。比如主键必须是唯一的，字段长度符合要求。除了数据库自身的完整性约束，还有一个是用户自定义的完整性。比如说转账的这个场景，A 账户余额减少 1000，B 账户余额只增加了 500，这个时 候因为两个操作都成功了，按照我们对原子性的定义，它是满足原子性的， 但是它没有 满足一致性，因为它导致了会计科目的不平衡。还有一种情况，A 账户余额为 0，如果这个时候转账成功了，A 账户的余额会变成 -1000，虽然它满足了原子性的，但是我们知道，借记卡的余额是不能够小于 0 的，所以 也违反了一致性。用户自定义的完整性通常要在代码中控制。 第三个，隔离性，Isolation，我们有了事务的定义以后，在数据库里面会有很多的 事务同时去操作我们的同一张表或者同一行数据，必然会产生一些并发或者干扰的操作， 那么我们对隔离性的定义，就是这些很多个的事务，对表或者行的并发操作，应该是透 明的，互相不干扰的。通过这种方式，我们最终也是保证业务数据的一致性。 第四个，持久性，Durable，事务的持久性是什么意思呢?我们对数据库的任意 的操作，增删改，只要事务提交成功，那么结果就是永久性的，不可能因为我们系统宕 机或者重启了数据库的服务器，它又恢复到原来的状态了。这个就是事务的持久性。持久性是通过 redo log 和 double write 双写缓冲来实现的，我们操作数据的时候，会先写到内存的 buffer pool 里面，同时记录 redo log，如果在刷盘之前出现异常，在 重启后就可以读取 redo log 的内容，写入到磁盘，保证数据的持久性。当然，恢复成功的前提是数据页本身没有被破坏，是完整的，这个通过双写缓冲 (double write)保证。原子性，隔离性，持久性，最后都是为了实现一致性。 数据库什么时候会出现事务无论是我们在 Navicat 的这种工具里面去操作，还是在我们的 Java 代码里面通过 API 去操作，还是加上@Transactional 的注解或者 AOP 配置，其实最终都是发送一个 指令到数据库去执行，Java 的 JDBC 只不过是把这些命令封装起来了。 我们先来看一下我们的操作环境。版本(5.7)，存储引擎(InnnoDB)，事务隔离 级别(RR)。 123select version(); ​show variables like '%engine%'; ​show global variables like "tx_isolation"; 执行这样一条更新语句的时候，它有事务吗? 1update student set sname = '猫老公 111' where id=1; 实际上，它自动开启了一个事务，并且提交了，所以最终写入了磁盘。这个是开启事务的第一种方式，自动开启和自动提交。InnoDB 里面有一个 autocommit 的参数(分成两个级别， session 级别和 global级别)。 1show variables like 'autocommit'; 它的默认值是 ON。autocommit 这个参数是什么意思呢?是否自动提交。如果它的 值是 true/on 的话，我们在操作数据的时候，会自动开启一个事务，和自动提交事务。否则，如果我们把 autocommit 设置成 false/off，那么数据库的事务就需要我们手 动地去开启和手动地去结束。手动开启事务也有几种方式，一种是用 begin;一种是用 start transaction。那么怎么结束一个事务呢?我们结束也有两种方式，第一种就是提交一个事务， commit;还有一种就是 rollback，回滚的时候，事务也会结束。还有一种情况，客户端 的连接断开的时候，事务也会结束。后面我们会讲到，当我们结束一个事务的时候，事务持有的锁就会被释放，无论是 提交还是回滚。我们用 begin 手工开启一个事务，执行第二个 update，但是数据没有写入磁盘，因 为事务还没有提交，这个时候 commit 一下，再刷新一下，OK，写入了。这个就是我们开启和结束事务的两种方式。 事务并发会带来什么问题?当很多事务并发地去操作数据库的表或者行的时候，如果没有我们刚才讲的事务的 Isolation 隔离性的时候，会带来哪些问题呢? 脏读 我们有两个事务，一个是 Transaction A，一个是 Transaction B，在第一个事务里 面，它首先通过一个 where id=1 的条件查询一条数据，返回 name=Ada，age=16 的 这条数据。然后第二个事务，它同样地是去操作 id=1 的这行数据，它通过一个 update 的语句，把这行 id=1 的数据的 age 改成了 18，但是注意，它没有提交。这个时候，在第一个事务里面，它再次去执行相同的查询操作，发现数据发生了变 化，获取到的数据 age 变成了 18。那么，这种在一个事务里面，由于其他的时候修改了 数据并且没有提交，而导致了前后两次读取数据不一致的情况，这种事务并发的问题， 我们把它定义成什么? 这个叫做脏读。如果在转账的案例里面，我们第一个事务基于读取到的第二个事务未提交的余额进行了操作，但是第二个事务进行了回滚，这个时候就会导致数据不一致。这种读取到其他事务未提交的数据的情况，我们把它叫做脏读。 我们再来看第二个。 不可重复读我们再来看第二个。 同样是两个事务，第一个事务通过 id=1 查询到了一条数据。然后在第二个事务里面 执行了一个 update 操作，这里大家注意一下，执行了 update 以后它通过一个 commit 提交了修改。然后第一个事务读取到了其他事务已提交的数据导致前后两次读取数据不 一致的情况，就像这里，age 到底是等于 16 还是 18，那么这种事务并发带来的问题， 我们把它叫做什么? 这种一个事务读取到了其他事务已提交的数据导致前后两次读取数据不一致的情 况，我们把它叫做不可重复读。 幻读 在第一个事务里面我们执行了一个范围查询，这个时候满足条件的数据只有一条。 在第二个事务里面，它插入了一行数据，并且提交了。重点:插入了一行数据。在第一 个事务里面再去查询的时候，它发现多了一行数据。这种情况，我们把它叫做什么呢?一个事务前后两次读取数据数据不一致，是由于其他事务插入数据造成的，这种情况我们把它叫做幻读。 不可重复读和幻读的区别在那里呢? 不可重复读是修改或者删除，幻读是插入。 小结小结:我们刚才讲了事务并发带来的三大问题，现在来给大家总结一下。无论是脏 读，还是不可重复读，还是幻读，它们都是数据库的读一致性的问题，都是在一个事务 里面前后两次读取出现了不一致的情况。读一致性的问题，必须要由数据库提供一定的事务隔离机制来解决。就像我们去饭 店吃饭，基本的设施和卫生保证都是饭店提供的。那么我们使用数据库，隔离性的问题 也必须由数据库帮助我们来解决。 SQL92标准所以，就有很多的数据库专家联合制定了一个标准，也就是说建议数据库厂商都按 照这个标准，提供一定的事务隔离级别，来解决事务并发的问题，这个就是 SQL92 标准。我们来看一下 SQL92 标准的官网。http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt 这里面有一张表格(搜索_iso)，里面定义了四个隔离级别，右边的 P1 P2 P3 就是 代表事务并发的 3 个问题，脏读，不可重复读，幻读。Possible 代表在这个隔离级别下， 这个问题有可能发生，换句话说，没有解决这个问题。Not Possible 就是解决了这个问题。 我们详细地分析一下这 4 个隔离级别是怎么定义的。第一个隔离级别叫做:Read Uncommitted(未提交读)，一个事务可以读取到其他事务未提交的数据，会出现脏读，所以叫做 RU，它没有解决任何的问题。第二个隔离级别叫做:Read Committed(已提交读)，也就是一个事务只能读取到其他事务已提交的数据，不能读取到其他事务未提交的数据，它解决了脏读的问题， 但是会出现不可重复读的问题。第三个隔离级别叫做:Repeatable Read (可重复读)，它解决了不可重复读的问题， 也就是在同一个事务里面多次读取同样的数据结果是一样的，但是在这个级别下，没有 定义解决幻读的问题。第四个隔离级别:Serializable(串行化)，在这个隔离级别里面，所有的事务都是串行执行的，也就是对数据的操作需要排队，已经不存在事务的并发操作了，所以它解决了所有的问题。 这个是 SQL92 的标准，但是不同的数据库厂商或者存储引擎的实现有一定的差异，比如 Oracle 里面就只有两种 RC(已提交读)和 Serializable(串行化)。那么 InnoDB 的实现又是怎么样的呢? MySQL InnoDB 对隔离级别的支持在 MySQL InnoDB 里面，不需要使用串行化的隔离级别去解决所有问题。那我们来 看一下 MySQL InnoDB 里面对数据库事务隔离级别的支持程度是什么样的。 InnoDB 支持的四个隔离级别和 SQL92 定义的基本一致，隔离级别越高，事务的并发度就越低。唯一的区别就在于，InnoDB 在 RR 的级别就解决了幻读的问题。这个也是 InnoDB 默认使用 RR 作为事务隔离级别的原因，既保证了数据的一致性，又支持较高的并发度。 两大实现方案如果要解决读一致性的问题，保证一个事务中前后两次读取数据 结果一致，实现事务隔离，应该怎么做?我们有哪一些方法呢?你的思路是什么样的呢?总体上来说，我们有两大类的方案。 LBCC第一种，我既然要保证前后两次读取数据一致，那么我读取数据的时候，锁定我要 操作的数据，不允许其他的事务修改就行了。这种方案我们叫做基于锁的并发控制 Lock Based Concurrency Control(LBCC)。如果仅仅是基于锁来实现事务隔离，一个事务读取的时候不允许其他时候修改，那就意味着不支持并发的读写操作，而我们的大多数应用都是读多写少的，这样会极大地 影响操作数据的效率。 MVCC所以我们还有另一种解决方案，如果要让一个事务前后两次读取的数据保持一致， 那么我们可以在修改数据的时候给它建立一个备份或者叫快照，后面再来读取这个快照 就行了。这种方案我们叫做多版本的并发控制 Multi Version Concurrency Control (MVCC)。MVCC 的核心思想是: 我可以查到在我这个事务开始之前已经存在的数据，即使它在后面被修改或者删除了。在我这个事务之后新增的数据，我是查不到的。问题:这个快照什么时候创建?读取数据的时候，怎么保证能读取到这个快照而不是最新的数据?这个怎么实现呢?InnoDB 为每行记录都实现了两个隐藏字段:DB_TRX_ID，6 字节:插入或更新行的最后一个事务的事务 ID，事务编号是自动递 增的(我们把它理解为创建版本号，在数据新增或者修改为新数据的时候，记录当前事 务 ID)。DB_ROLL_PTR，7 字节:回滚指针(我们把它理解为删除版本号，数据被删除或记 录为旧数据的时候，记录当前事务 ID)。我们把这两个事务 ID 理解为版本号。https://www.processon.com/view/link/5d29999ee4b07917e2e09298 MVCC 演示图 第一个事务，初始化数据(检查初始数据)此时的数据，创建版本是当前事务 ID，删除版本为空:第二个事务，执行第 1 次查询，读取到两条原始数据，这个时候事务 ID 是 2:第三个事务，插入数据:此时的数据，多了一条 tom，它的创建版本号是当前事务编号，3:第二个事务，执行第 2 次查询: MVCC 的查找规则:只能查找创建时间小于等于当前事务 ID 的数据，和删除时间大于当前事务 ID 的行(或未删除)。也就是不能查到在我的事务开始之后插入的数据，tom 的创建 ID 大于 2，所以还是只能查到两条数据。 第四个事务，删除数据，删除了 id=2 jack 这条记录: 1234-- Transaction 4begin;delete from mvcctest where id=2; commit; 此时的数据，jack 的删除版本被记录为当前事务 ID，4，其他数据不变: 在第二个事务中，执行第 3 次查询: 12-- Transaction 2select * from mvcctest ; -- (3) 第三次查询 查找规则:只能查找创建时间小于等于当前事务 ID 的数据，和删除时间大于当前事务 ID 的行(或未删除)。也就是，在我事务开始之后删除的数据，所以 jack 依然可以查出来。所以还是这两条数据。 第五个事务，执行更新操作，这个事务事务 ID 是 5: 12-- Transaction 4update mvcctest set name ='盆鱼宴' where id=1; 此时的数据，更新数据的时候，旧数据的删除版本被记录为当前事务 ID 5(undo)， 产生了一条新数据，创建 ID 为当前事务 ID 5: 第二个事务，执行第 4 次查询: 12-- Transaction 2select * from mvcctest ; -- (4) 第四次查询 查找规则:只能查找创建时间小于等于当前事务 ID 的数据，和删除时间大于当前事务 ID 的行(或未删除)。因为更新后的数据 penyuyan 创建版本大于 2，代表是在事务之后增加的，查不出来。而旧数据 qingshan 的删除版本大于 2，代表是在事务之后删除的，可以查出来。通过以上演示我们能看到，通过版本号的控制，无论其他事务是插入、修改、删除， 第一个事务查询到的数据都没有变化。在 InnoDB 中，MVCC 是通过 Undo log 实现的。Oracle、Postgres 等等其他数据库都有 MVCC 的实现。需要注意，在 InnoDB 中，MVCC 和锁是协同使用的，这两种方案并不是互斥的。 第一大类解决方案是锁，锁又是怎么实现读一致性的呢? MySQL InnoDB 锁的基本类型https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html官网把锁分成了 8 类。所以我们把前面的两个行级别的锁(Shared and Exclusive Locks)，和两个表级别的锁(Intention Locks)称为锁的基本模式。后面三个 Record Locks、Gap Locks、Next-Key Locks，我们把它们叫做锁的算法， 也就是分别在什么情况下锁定什么范围。 锁的粒度我们讲到 InnoDB 里面既有行级别的锁，又有表级别的锁，我们先来分析一下这两种锁定粒度的一些差异。表锁，顾名思义，是锁住一张表;行锁就是锁住表里面的一行数据。锁定粒度，表锁肯定是大于行锁的。那么加锁效率，表锁应该是大于行锁还是小于行锁呢?大于。为什么?表锁只需要直接锁住这张表就行了，而行锁，还需要在表里面去检索这一行数据，所以表锁的加锁效率更高。 第二个冲突的概率?表锁的冲突概率比行锁大，还是小?大于，因为当我们锁住一张表的时候，其他任何一个事务都不能操作这张表。但是 我们锁住了表里面的一行数据的时候，其他的事务还可以来操作表里面的其他没有被锁定的行，所以表锁的冲突概率更大。表锁的冲突概率更大，所以并发性能更低，这里并发性能就是小于。innoDB 里面我们知道它既支持表锁又支持行锁，另一个常用的存储引擎 MyISAM 支 持什么粒度的锁?这是第一个问题。第二个就是 InnoDB 已经支持行锁了，那么它也可以通过把表里面的每一行都锁住来实现表锁，为什么还要提供表锁呢?要搞清楚这个问题，我们就要来了解一下 InnoDB 里面的基本的锁的模式(lock mode)，这里面有两个行锁和两个表锁。 共享锁第一个行级别的锁就是我们在官网看到的 Shared Locks (共享锁)，我们获取了 一行数据的读锁以后，可以用来读取数据，所以它也叫做读锁，注意不要在加上了读锁 以后去写数据，不然的话可能会出现死锁的情况。而且多个事务可以共享一把读锁。那怎么给一行数据加上读锁呢? 我们可以用 select …… lock in share mode; 的方式手工加上一把读锁。 释放锁有两种方式，只要事务结束，锁就会自动释放，包括提交事务和结束事务。我们也来验证一下，看看共享锁是不是可以重复获取。 排它锁第二个行级别的锁叫做 Exclusive Locks(排它锁)，它是用来操作数据的，所以又 叫做写锁。只要一个事务获取了一行数据的排它锁，其他的事务就不能再获取这一行数 据的共享锁和排它锁。排它锁的加锁方式有两种，第一种是自动加排他锁。我们在操作数据的时候，包括 增删改，都会默认加上一个排它锁。还有一种是手工加锁，我们用一个 FOR UPDATE 给一行数据加上一个排它锁，这个 无论是在我们的代码里面还是操作数据的工具里面，都比较常用。释放锁的方式跟前面是一样的。排他锁的验证: 这个是两个行锁，接下来就是两个表锁。 意向锁意向锁是什么呢?我们好像从来没有听过，也从来没有使用过，其实他们是由数据库自己维护的。也就是说，当我们给一行数据加上共享锁之前，数据库会自动在这张表上面加一个意向共享锁。当我们给一行数据加上排他锁之前，数据库会自动在这张表上面加一个意向排他锁。反过来说: 如果一张表上面至少有一个意向共享锁，说明有其他的事务给其中的某些数据行加上了共享锁。如果一张表上面至少有一个意向排他锁，说明有其他的事务给其中的某些数据行加上了排他锁。 那么这两个表级别的锁存在的意义是什么呢?第一个，我们有了表级别的锁，在 InnoDB 里面就可以支持更多粒度的锁。它的第二个作用，我们想一下，如果说没有意向 锁的话，当我们准备给一张表加上表锁的时候，我们首先要做什么?是不是必须先要去 判断有没其他的事务锁定了其中了某些行?如果有的话，肯定不能加上表锁。那么这个 时候我们就要去扫描整张表才能确定能不能成功加上一个表锁，如果数据量特别大，比 如有上千万的数据的时候，加表锁的效率是不是很低? 但是我们引入了意向锁之后就不一样了。我只要判断这张表上面有没有意向锁，如 果有，就直接返回失败。如果没有，就可以加锁成功。所以 InnoDB 里面的表锁，我们 可以把它理解成一个标志。就像火车上厕所有没有人使用的灯，是用来提高加锁的效率的。以上就是 MySQL 里面的 4 种基本的锁的模式，或者叫做锁的类型。到这里我们要思考两个问题，首先，锁的作用是什么?它跟 Java 里面的锁是一样的，是为了解决资源竞争的问题，Java 里面的资源是对象，数据库的资源就是数据表或者数据行。所以锁是用来解决事务对数据的并发访问的问题的。那么，锁到底锁住了什么呢?当一个事务锁住了一行数据的时候，其他的事务不能操作这一行数据，那它到底是锁住了这一行数据，还是锁住了这一个字段，还是锁住了别的什么东西呢? 行锁的原理没有索引的表(假设锁住记录)首先我们有三张表，一张没有索引的 t1，一张有主键索引的 t2，一张有唯一索引的 t3。我们先假设 InnoDB 的锁锁住了是一行数据或者一条记录。 我们先来看一下 t1 的表结构，它有两个字段，int 类型的 id 和 varchar 类型的 name。 里面有 4 条数据，1、2、3、4。现在我们在两个会话里面手工开启两个事务。在第一个事务里面，我们通过 where id =1 锁住第一行数据。在第二个事务里面，我们尝试给 id=3 的这一行数据加锁，大家觉得能成功吗? 很遗憾，我们看到红灯亮起，这个加锁的操作被阻塞了。这就有点奇怪了，第一个事务锁住了 id=1 的这行数据，为什么我不能操作 id=3 的数据呢? 我们再来操作一条不存在的数据，插入 id=5。它也被阻塞了。实际上这里整张表都被锁住了。所以，我们的第一个猜想被推翻了，InnoDB 的锁锁住的应该不是 Record。 那为什么在没有索引或者没有用到索引的情况下，会锁住整张表?这个问题我们先留在这里。 有主键索引的表我们看一下 t2 的表结构。字段是一样的，不同的地方是 id 上创建了一个主键索引。 里面的数据是 1、4、7、10。 第一种情况，使用相同的 id 值去加锁，冲突;使用不同的 id 加锁，可以加锁成功。 那么，既然不是锁定一行数据，有没有可能是锁住了 id 的这个字段呢? 唯一索引(假设锁住字段)我们看一下 t3 的表结构。字段还是一样的， id 上创建了一个主键索引，name 上 创建了一个唯一索引。里面的数据是 1、4、7、10。 在第一个事务里面，我们通过 name 字段去锁定值是 4 的这行数据。在第二个事务里面，尝试获取一样的排它锁，肯定是失败的，这个不用怀疑。在这里我们怀疑 InnoDB 锁住的是字段，所以这次我换一个字段，用 id=4 去给这行数据加锁，大家觉得能成功吗? 很遗憾，又被阻塞了，说明锁住的是字段的这个推测也是错的，否则就不会出现第 一个事务锁住了 name，第二个字段锁住 id 失败的情况。 既然锁住的不是 record，也不是 column，InnoDB 里面锁住的到底是什么呢?在这 三个案例里面，我们要去分析一下他们的差异在哪里，也就是这三张表的结构，是什么 区别导致了加锁的行为的差异?其实答案就是索引。InnoDB 的行锁，就是通过锁住索引来实现的。 那么我们还有两个问题没有解决:1、为什么表里面没有索引的时候，锁住一行数据会导致锁表? 或者说，如果锁住的是索引，一张表没有索引怎么办? 所以，一张表有没有可能没有索引?1)如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。2)如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索引作为主键索引。3)如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐藏的聚集索引，它会随着行记录的写入而主键递增。所以，为什么锁表，是因为查询没有使用索引，会进行全表扫描，然后把每一个隐藏的聚集索引都锁住了。 2、为什么通过唯一索引给数据行加锁，主键索引也会被锁住?大家还记得在 InnoDB 里面，当我们使用辅助索引的时候，它是怎么检索数据的吗? 辅助索引的叶子节点存储的是什么内容?在辅助索引里面，索引存储的是二级索引和主键的值。比如 name=4，存储的是 name 的索引和主键 id 的值 4。而主键索引里面除了索引之外，还存储了完整的数据。所以我们通过辅助索引锁定 一行数据的时候，它跟我们检索数据的步骤是一样的，会通过主键值找到主键索引，然后也锁定。 锁的算法记录锁第一种情况，当我们对于唯一性的索引(包括唯一索引和主键索引)使用等值查询，精准匹配到一条记录的时候，这个时候使用的就是记录锁。比如 where id=1 4 7 10 。这个演示我们在前面已经看过了。我们使用不同的 key 去加锁，不会冲突，它只锁住这个 record。 间隙锁第二种情况，当我们查询的记录不存在，没有命中任何一个 record，无论是用等值 查询还是范围查询的时候，它使用的都是间隙锁。举个例子，where id &gt;4 and id &lt;7，where id = 6。重复一遍，当查询的记录不存在的时候，使用间隙锁。注意，间隙锁主要是阻塞插入 insert。相同的间隙锁之间不冲突。 Gap Lock 只在 RR 中存在。如果要关闭间隙锁，就是把事务隔离级别设置成 RC， 并且把 innodb_locks_unsafe_for_binlog 设置为 ON。这种情况下除了外键约束和唯一性检查会加间隙锁，其他情况都不会用间隙锁。 临键锁第三种情况，当我们使用了范围查询，不仅仅命中了 Record 记录，还包含了 Gap 间隙，在这种情况下我们使用的就是临键锁，它是 MySQL 里面默认的行锁算法，相当于 记录锁加上间隙锁。其他两种退化的情况:唯一性索引，等值查询匹配到一条记录的时候，退化成记录锁。没有匹配到任何记录的时候，退化成间隙锁。 比如我们使用&gt;5 &lt;9， 它包含了记录不存在的区间，也包含了一个 Record 7。 临键锁，锁住最后一个 key 的下一个左开右闭的区间。 12select * from t2 where id &gt;5 and id &lt;=7 for update; -- 锁住(4,7]和(7,10]select * from t2 where id &gt;8 and id &lt;=10 for update; -- 锁住 (7,10]，(10,+∞) 为什么要锁住下一个左开右闭的区间?——就是为了解决幻读的问题。 小结:隔离级别的实现所以，我们再回过头来看下这张图片，为什么 InnoDB 的 RR 级别能够解决幻读的问题，就是用临键锁实现的。 我们再回过头来看下这张图片，这个就是 MySQL InnoDB 里面事务隔离级别的实现。 最后我们来总结一下四个事务隔离级别的实现: Read UncommitedRU 隔离级别:不加锁。 SerializableSerializable 所有的 select 语句都会被隐式的转化为 select … in share mode，会 和 update、delete 互斥。这两个很好理解，主要是 RR 和 RC 的区别? Repeatable ReadRR 隔离级别下，普通的 select 使用快照读(snapshot read)，底层使用 MVCC 来实现。 加锁的 select(select … in share mode / select … for update)以及更新操作 update, delete 等语句使用当前读(current read)，底层使用记录锁、或者间隙锁、 临键锁。 Read CommitedRC 隔离级别下，普通的 select 都是快照读，使用 MVCC 实现。 加锁的 select 都使用记录锁，因为没有 Gap Lock。除了两种特殊情况——外键约束检查(foreign-key constraint checking)以及重复 键检查(duplicate-key checking)时会使用间隙锁封锁区间。所以 RC 会出现幻读的问题。 事务隔离级别怎么选?RU 和 Serializable 肯定不能用。为什么有些公司要用 RC，或者说网上有些文章推 荐有 RC?RC 和 RR 主要有几个区别:1、 RR 的间隙锁会导致锁定范围的扩大。2、 条件列未使用到索引，RR 锁表，RC 锁行。3、 RC 的“半一致性”(semi-consistent)读可以增加 update 操作的并发性。在 RC 中，一个 update 语句，如果读到一行已经加锁的记录，此时 InnoDB 返回记录最近提交的版本，由 MySQL 上层判断此版本是否满足 update 的 where 条件。若满足(需要更新)，则 MySQL 会重新发起一次读操作，此时会读取行的最新版本(并加锁)。 实际上，如果能够正确地使用锁(避免不使用索引去加锁)，只锁定需要的数据， 用默认的 RR 级别就可以了。 在我们使用锁的时候，有一个问题是需要注意和避免的，我们知道，排它锁有互斥的特性。一个事务或者说一个线程持有锁的时候，会阻止其他的线程获取锁，这个时候会造成阻塞等待，如果循环等待，会有可能造成死锁。 这个问题我们需要从几个方面来分析，一个是锁为什么不释放，第二个是被阻塞了 怎么办，第三个死锁是怎么发生的，怎么避免。 死锁锁的释放与阻塞回顾:锁什么时候释放? 事务结束(commit，rollback);客户端连接断开。 如果一个事务一直未释放锁，其他事务会被阻塞多久?会不会永远等待下去?如果 是，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占 用大量计算机资源，造成严重性能问题，甚至拖跨数据库。 [Err] 1205 - Lock wait timeout exceeded; try restarting transactionMySQL有一个参数来控制获取锁的等待时间，默认是 50 秒。 1show VARIABLES like 'innodb_lock_wait_timeout'; 对于死锁，是无论等多久都不能获取到锁的，这种情况，也需要等待 50 秒钟吗?那 不是白白浪费了 50 秒钟的时间吗?我们先来看一下什么时候会发生死锁。 死锁的发生和检测在第一个事务中，检测到了死锁，马上退出了，第二个事务获得了锁，不需要等待 50 秒:[Err] 1213 - Deadlock found when trying to get lock; try restarting transaction 为什么可以直接检测到呢?是因为死锁的发生需要满足一定的条件，所以在发生死 锁时，InnoDB 一般都能通过算法(wait-for graph)自动检测到。 那么死锁需要满足什么条件?死锁的产生条件:因为锁本身是互斥的，(1)同一时刻只能有一个事务持有这把锁，(2)其他的事务需要在这个事务释放锁之后才能获取锁，而不可以强行剥夺。(3)当多个事务形成等待环路的时候，即发生死锁。 举例:理发店有两个总监。一个负责剪头的 Tony 总监，一个负责洗头的 Kelvin 总监。Tony 不能同时给两个人剪头，这个就叫互斥。Tony 在给别人在剪头的时候，你不能让他停下来帮你剪头，这个叫不能强行剥夺。如果 Tony 的客户对 Kelvin 总监说:你不帮我洗头我怎么剪头?Kelvin 的客户对 Tony 总监说:你不帮我剪头我怎么洗头?这个就叫形成等待环路。 如果锁一直没有释放，就有可能造成大量阻塞或者发生死锁，造成系统吞吐量下降，这时候就要查看是哪些事务持有了锁。 查看锁信息(日志)SHOW STATUS 命令中，包括了一些行锁的信息: 1show status like 'innodb_row_lock_%'; Innodb_row_lock_current_waits:当前正在等待锁定的数量;Innodb_row_lock_time :从系统启动到现在锁定的总时间长度，单位 ms;Innodb_row_lock_time_avg :每次等待所花平均时间;Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间;Innodb_row_lock_waits :从系统启动到现在总共等待的次数。 SHOW 命令是一个概要信息。InnoDB 还提供了三张表来分析事务与锁的情况: 1select * from information_schema.INNODB_TRX; -- 当前运行的所有事务 ，还有具体的语句 1select * from information_schema.INNODB_LOCKS; -- 当前出现的锁 1select * from information_schema.INNODB_LOCK_WAITS; -- 锁等待的对应关系 找出持有锁的事务之后呢?如果一个事务长时间持有锁不释放，可以 kill 事务对应的线程 ID，也就是 INNODB_TRX 表中的 trx_mysql_thread_id，例如执行 kill 4，kill 7，kill 8。当然，死锁的问题不能每次都靠 kill 线程来解决，这是治标不治本的行为。我们应该 尽量在应用端，也就是在编码的过程中避免。有哪些可以避免死锁的方法呢? 死锁的避免1、 在程序中，操作多张表时，尽量以相同的顺序来访问(避免形成等待环路);2、 批量操作单张表数据的时候，先对数据进行排序(避免形成等待环路);3、 申请足够级别的锁，如果要操作数据，就申请排它锁;4、 尽量使用索引访问数据，避免没有 where 条件的操作，避免锁表;5、 如果可以，大事务化成小事务;6、 使用等值查询而不是范围查询查询数据，命中记录，避免间隙锁对并发的影响。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq]]></title>
    <url>%2F2020%2F03%2F21%2Frabbitmq%2F</url>
    <content type="text"><![CDATA[什么是mq消息队列，又叫做消息中间件。是指用高效可靠的消息传递机制进行与平台无关的 数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息队列模 型，可以在分布式环境下扩展进程的通信(维基百科)。 为什么要用mq1，实现异步通信。2，实现系统解耦。3，实现流量削峰。 总结起来:1) 对于数据量大或者处理耗时长的操作，我们可以引入 MQ 实现异步通信，减少客户端的等待，提升响应速度。2) 对于改动影响大的系统之间，可以引入 MQ 实现解耦，减少系统之间的直接依赖。3) 对于会出现瞬间的流量峰值的系统，我们可以引入 MQ 实现流量削峰，达到保护应用和数据库的目的。 RabbitMQ 简介基本特性官网 https://www.rabbitmq.com/getstarted.html 高可靠:RabbitMQ 提供了多种多样的特性让你在可靠性和性能之间做出权衡，包 括持久化、发送应答、发布确认以及高可用性。 灵活的路由:通过交换机(Exchange)实现消息的灵活路由。 支持多客户端:对主流开发语言(Python、Java、Ruby、PHP、C#、JavaScript、 Go、Elixir、Objective-C、Swift 等)都有客户端实现。 集群与扩展性:多个节点组成一个逻辑的服务器，支持负载。 高可用队列:通过镜像队列实现队列中数据的复制。 权限管理:通过用户与虚拟机实现权限管理。 插件系统:支持各种丰富的插件扩展，同时也支持自定义插件。 与 Spring 集成:Spring 对 AMQP 进行了封装。 AMQP 协议AMQP:高级消息队列协议，是一个工作于应用层的协议，最新的版本是 1.0 版本。除了 RabbitMQ 之外，AMQP 的实现还有 OpenAMQ、Apache Qpid、Redhat Enterprise MRG、AMQP Infrastructure、ØMQ、Zyre。 除了 AMQP 之外，RabbitMQ 支持多种协议，STOMP、MQTT、HTTP and WebSockets。可以使用 WireShark 等工具对 RabbitMQ 通信的 AMQP 协议进行抓包。 工作模型由于 RabbitMQ 实现了 AMQP 协议，所以 RabbitMQ 的工作模型也是基于 AMQP 的。 Broker我们要使用 RabbitMQ 来收发消息，必须要安装一个 RabbitMQ 的服务，可以安装 在 Windows 上面也可以安装在 Linux 上面，默认是 5672 的端口。这台 RabbitMQ 的 服务器我们把它叫做 Broker，中文翻译是代理/中介，因为 MQ 服务器帮助我们做的事 情就是存储、转发消息。 Connection无论是生产者发送消息，还是消费者接收消息，都必须要跟 Broker 之间建立一个连接，这个连接是一个 TCP 的长连接。 Channel如果所有的生产者发送消息和消费者接收消息，都直接创建和释放 TCP 长连接的话， 对于 Broker 来说肯定会造成很大的性能损耗，因为 TCP 连接是非常宝贵的资源，创建和 释放也要消耗时间。所以在 AMQP 里面引入了 Channel 的概念，它是一个虚拟的连接。我们把它翻译成通道，或者消息信道。这样我们就可以在保持的 TCP 长连接里面去创建和释放 Channel，大大了减少了资源消耗。另外一个需要注意的是，Channel 是 RabbitMQ 原 生 API 里面的最重要的编程接口，也就是说我们定义交换机、队列、绑定关系，发送消 息消费消息，调用的都是 Channel 接口上的方法。https://stackoverflow.com/questions/18418936/rabbitmq-and-relationship- between-channel-and-connection Queue现在我们已经连到 Broker 了，可以收发消息了。在其他一些 MQ 里面，比如 ActiveMQ 和 Kafka，我们的消息都是发送到队列上的。 队列是真正用来存储消息的，是一个独立运行的进程，有自己的数据库(Mnesia)。 消费者获取消息有两种模式，一种是 Push 模式，只要生产者发到服务器，就马上推 送给消费者。另一种是 Pull 模式，消息存放在服务端，只有消费者主动获取才能拿到消 息。消费者需要写一个 while 循环不断地从队列获取消息吗?不需要，我们可以基于事 件机制，实现消费者对队列的监听。 由于队列有 FIFO 的特性，只有确定前一条消息被消费者接收之后，才会把这条消息 从数据库删除，继续投递下一条消息。 Excahnge在 RabbitMQ 里面永远不会出现消息直接发送到队列的情况。因为在 AMQP 里面 引入了交换机(Exchange)的概念，用来实现消息的灵活路由。 交换机是一个绑定列表，用来查找匹配的绑定关系。 队列使用绑定键(Binding Key)跟交换机建立绑定关系。 生产者发送的消息需要携带路由键(Routing Key)，交换机收到消息时会根据它保存的绑定列表，决定将消息路由到哪些与它绑定的队列上。 注意:交换机与队列、队列与消费者都是多对多的关系。 Vhost我们每个需要实现基于 RabbitMQ 的异步通信的系统，都需要在服务器上创建自己要用到的交换机、队列和它们的绑定关系。如果某个业务系统不想跟别人混用一个系统， 怎么办?再采购一台硬件服务器单独安装一个 RabbitMQ 服务?这种方式成本太高了。 在同一个硬件服务器上安装多个 RabbitMQ 的服务呢?比如再运行一个 5673 的端口? 没有必要，因为 RabbitMQ 提供了虚拟主机 VHOST。 VHOST 除了可以提高硬件资源的利用率之外，还可以实现资源的隔离和权限的控 制。它的作用类似于编程语言中的 namespace 和 package，不同的 VHOST 中可以有 同名的 Exchange 和 Queue，它们是完全透明的。 这个时候，我们可以为不同的业务系统创建不同的用户(User)，然后给这些用户 分配 VHOST 的权限。比如给风控系统的用户分配风控系统的 VHOST 的权限，这个用户 可以访问里面的交换机和队列。给超级管理员分配所有 VHOST 的权限。 路由方式RabbitMQ 引入 Exchange 是为了实现消息的灵活路由，到底有哪些路由方式? 直连 Direct队列与直连类型的交换机绑定，需指定一个精确的绑定键。 生产者发送消息时会携带一个路由键。只有当路由键与其中的某个绑定键完全匹配 时，这条消息才会从交换机路由到满足路由关系的此队列上。 主题 Topic队列与主题类型的交换机绑定时，可以在绑定键中使用通配符。两个通配符: 12# 0个或者多个单词* 不多不少一个单词 单词(word)指的是用英文的点“.”隔开的字符。例如 abc.def 是两个单词。 解读:第一个队列支持路由键以 junior 开头的消息路由，后面可以有单词，也可以 没有。第二个队列支持路由键以 netty 开头，并且后面是一个单词的消息路由。第三个队列支持路由键以 jvm 结尾，并且前面是一个单词的消息路由。 广播 Fanout主题类型的交换机与队列绑定时，不需要指定绑定键。因此生产者发送消息到广播类型的交换机上，也不需要携带路由键。消息达到交换机时，所有与之绑定了的队列， 都会收到相同的消息的副本。 例如:channel.basicPublish(“MY_FANOUT_EXCHANGE”, “”, “msg 4”); 收到 msg 4。 RabbitMQ 进阶知识2.1. TTL(Time To Live)有两种设置方式: 1) 通过队列属性设置消息过期时间 所有队列中的消息超过时间未被消费时，都会过期。2) 设置单条消息的过期时间 在发送消息的时候指定消息属性。 如果同时指定了 Message TTL 和 Queue TTL，则小的那个时间生效。 死信队列消息在某些情况下会变成死信(Dead Letter)。队列在创建的时候可以指定一个死信交换机 DLX(Dead Letter Exchange)。 死信交换机绑定的队列被称为死信队列 DLQ(Dead Letter Queue)，DLX 实际上 也是普通的交换机，DLQ 也是普通的队列(例如替补球员也是普通球员)。 什么情况下消息会变成死信? 消息被消费者拒绝并且未设置重回队列:(NACK || Reject ) &amp;&amp; requeue == false 消息过期 队列达到最大长度，超过了 Max length(消息数)或者 Max length bytes (字节数)，最先入队的消息会被发送到 DLX。 死信队列如何使用?1、声明原交换机(GP_ORI_USE_EXCHANGE)、原队列(GP_ORI_USE_QUEUE)，相互绑定。队列中的消息 10 秒钟过期，因为没有消费者，会变成死信。指定原队列的死信交换机(GP_DEAD_LETTER_EXCHANGE)。 12345678910111213141516@Bean("oriUseExchange")public DirectExchange exchange() &#123; return new DirectExchange("GP_ORI_USE_EXCHANGE", true, false, new HashMap&lt;&gt;());&#125;​@Bean("oriUseQueue") public Queue queue() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put("x-message-ttl", 10000); // 10 秒钟后成为死信 map.put("x-dead-letter-exchange", "GP_DEAD_LETTER_EXCHANGE"); // 队列中的消息变成死信后，进入死信交换机 return new Queue("GP_ORI_USE_QUEUE", true, false, false, map);&#125;​@Beanpublic Binding binding(@Qualifier("oriUseQueue") Queue queue,@Qualifier("oriUseExchange") DirectExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with("gupao.ori.use");&#125; 2 、 声 明 死 信 交 换 机 ( GP_DEAD_LETTER_EXCHANGE ) 、 死 信 队 列 (GP_DEAD_LETTER_QUEUE)，相互绑定 123456789101112@Bean("deatLetterExchange")public TopicExchange deadLetterExchange() &#123; return new TopicExchange("GP_DEAD_LETTER_EXCHANGE", true, false, new HashMap&lt;&gt;());&#125;@Bean("deatLetterQueue")public Queue deadLetterQueue() &#123; return new Queue("GP_DEAD_LETTER_QUEUE", true, false, false, new HashMap&lt;&gt;());&#125;@Beanpublic Binding bindingDead(@Qualifier("deatLetterQueue") Queue queue,@Qualifier("deatLetterExchange") TopicExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with("#");// 无条件路由 &#125; 3、最终消费者监听死信队列。4、生产者发送消息。 延迟队列我们在实际业务中有一些需要延时发送消息的场景，例如:1、 家里有一台智能热水器，需要在 30 分钟后启动2、 未付款的订单，15 分钟后关闭 RabbitMQ 本身不支持延迟队列，总的来说有三种实现方案:1、 先存储到数据库，用定时任务扫描2、 利用 RabbitMQ 的死信队列(Dead Letter Queue)实现3、 利用 rabbitmq-delayed-message-exchange 插件 服务端流控 (Flow Control)当 RabbitMQ 生产 MQ 消息的速度远大于消费消息的速度时，会产生大量的消息堆积，占用系统资源，导致机器的性能下降。我们想要控制服务端接收的消息的数量，应该怎么做呢?队列有两个控制长度的属性: x-max-length:队列中最大存储最大消息数，超过这个数量，队头的消息会被丢弃。 x-max-length-bytes:队列中存储的最大消息容量(单位 bytes)，超过这个容量，队头的消息会被丢弃。 需要注意的是，设置队列长度只在消息堆积的情况下有意义，而且会删除先入队的 消息，不能真正地实现服务端限流。 内存控制RabbitMQ 会在启动时检测机器的物理内存数值。默认当 MQ 占用 40% 以上内 存时，MQ 会主动抛出一个内存警告并阻塞所有连接(Connections)。可以通过修改 rabbitmq.config 文件来调整内存阈值，默认值是 0.4，如下所示: 1[&#123;rabbit, [&#123;vm_memory_high_watermark, 0.4&#125;]&#125;]. 也可以用命令动态设置，如果设置成 0，则所有的消息都不能发布。 1rabbitmqctl set_vm_memory_high_watermark 0.3 磁盘控制另一种方式是通过磁盘来控制消息的发布。当磁盘空间低于指定的值时(默认 50MB)，触发流控措施。例如:指定为磁盘的 30%或者 2GB:https://www.rabbitmq.com/configure.html 12disk_free_limit.relative = 3.0 disk_free_limit.absolute = 2GB 消费端限流https://www.rabbitmq.com/consumer-prefetch.html默认情况下，如果不进行配置，RabbitMQ 会尽可能快速地把队列中的消息发送到 消费者。因为消费者会在本地缓存消息，如果消息数量过多，可能会导致 OOM 或者影 响其他进程的正常运行。在消费者处理消息的能力有限，例如消费者数量太少，或者单条消息的处理时间过长的情况下，如果我们希望在一定数量的消息消费完之前，不再推送消息过来，就要用到消费端的流量限制措施。可以基于 Consumer 或者 channel 设置 prefetch count 的值，含义为 Consumer端的最大的 unacked messages 数目。当超过这个数值的消息未被确认，RabbitMQ 会 停止投递新的消息给该消费者。 12channel.basicQos(2); // 如果超过 2 条消息没有发送 ACK，当前消费者不再接受队列消息channel.basicConsume(QUEUE_NAME, false, consumer); SimpleMessageListenerContainer 1container.setPrefetchCount(2); Spring Boot 配置: 1spring.rabbitmq.listener.simple.prefetch=2 Spring AMQPSpring AMQP 介绍Spring 封装 RabbitMQ 的时候，它做了什么事情? 管理对象(队列、交换机、绑定) 封装方法(发送消息、接收消息) Spring AMQP 是对 Spring 基于 AMQP 的消息收发解决方案，它是一个抽象层， 不依赖于特定的 AMQP Broker 实现和客户端的抽象，所以可以很方便地替换。比如我 们可以使用 spring-rabbit 来实现。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt;&lt;/dependency&gt; 包括 3 个 jar 包: Amqp-client-3.3.4.jar Spring-amqp.jar Spring.rabbit.jar rabbitmq消息可靠性投递 在我们使用 RabbitMQ 收发消息的时候，有几个主要环节: 代表消息从生产者发送到 Broker.生产者把消息发到 Broker 之后，怎么知道自己的消息有没有被 Broker 成功接 收? 代表消息从 Exchange 路由到 Queue.Exchange 是一个绑定列表，如果消息没有办法路由到正确的队列，会发生什么事情?应该怎么处理? 代表消息在 Queue 中存储队列是一个独立运行的服务，有自己的数据库(Mnesia)，它是真正用来存储消 息的。如果还没有消费者来消费，那么消息要一直存储在队列里面。如果队列出了问 题，消息肯定会丢失。怎么保证消息在队列稳定地存储呢? 代表消费者订阅 Queue 并消费消息队列的特性是什么?FIFO。队列里面的消息是一条一条的投递的，也就是说，只有上一条消息被消费者接收以后，才能把这一条消息从数据库删掉，继续投递下一条 消息。那么问题来了，Broker 怎么知道消费者已经接收了消息呢? 消息发送到 RabbitMQ 服务器第一个环节是生产者发送消息到 Broker。可能因为网络或者 Broker 的问题导致消息 发送失败，生产者不能确定 Broker 有没有正确的接收。 在 RabbitMQ 里面提供了两种机制服务端确认机制，也就是在生产者发送消息给 RabbitMQ 的服务端的时候，服务端会通过某种方式返回一个应答，只要生产者收到了 这个应答，就知道消息发送成功了。 第一种是 Transaction(事务)模式，第二种 Confirm(确认)模式。 Transaction(事务)模式事务模式怎么使用呢? 我们通过一个 channel.txSelect()的方法把信道设置成事务模式，然后就可以发布消 息给 RabbitMQ 了，如果 channel.txCommit();的方法调用成功，就说明事务提交成功， 则消息一定到达了 RabbitMQ 中。 如果在事务提交执行之前由于 RabbitMQ 异常崩溃或者其他原因抛出异常，这个时 候我们便可以将其捕获，进而通过执行 channel.txRollback()方法来实现事务回滚。流程图如下： 在事务模式里面，只有收到了服务端的 Commit-OK 的指令，才能提交成功。所以可以解决生产者和服务端确认的问题。但是事务模式有一个缺点，它是阻塞的，一条消息没有发送完毕，不能发送下一条消息，它会榨干 RabbitMQ 服务器的性能。所以不建 议大家在生产环境使用。 SpringBoot中的设置： 1rabbitTemplate.setChannelTransacted(true); Confirm(确认)模式确认模式有三种，一种是普通确认模式。 在生产者这边通过调用 channel.confirmSelect()方法将信道设置为 Confirm 模式， 然后发送消息。一旦消息被投递到所有匹配的队列之后，RabbitMQ 就会发送一个确认 (Basic.Ack)给生产者，也就是调用 channel.waitForConfirms()返回 true，这样生产 者就知道消息被服务端接收了。 这种发送 1 条确认 1 条的方式消息还不是太高，所以我们还有一种批量确认的方式。 批量确认，就是在开启 Confirm 模式后，先发送一批消息。只要 channel.waitForConfirmsOrDie();方法没有抛出异常，就代表消息都被服务端接收了。 批量确认的方式比单条确认的方式效率要高，但是也有两个问题，第一个就是批量 的数量的确定。对于不同的业务，到底发送多少条消息确认一次?数量太少，效率提升 不上去。数量多的话，又会带来另一个问题，比如我们发 1000 条消息才确认一次，如果 前面 999 条消息都被服务端接收了，如果第 1000 条消息被拒绝了，那么前面所有的消 息都要重发。 异步确认模式。 异步确认模式需要添加一个 ConfirmListener，并且用一个 SortedSet 来维护没有 被确认的消息。Confirm 模式是在 Channel 上开启的，因为 RabbitTemplate 对 Channel 进行了封 装，叫做 ConfimrCallback。 123456789rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (!ack) &#123; System.out.println("发送消息失败:" + cause); throw new RuntimeException("发送异常:" + cause); &#125; &#125;&#125;); 消息从 Exchange 路由到 Queue第二个环节就是消息从交换机路由到队列。在什么情况下，消息会无法路由到正确 的队列?可能因为路由键错误，或者队列不存在。 我们有两种方式处理无法路由的消息，一种就是让服务端重发给生产者，一种是让 交换机路由到另一个备份的交换机。 消息回发的方式:使用 mandatory 参数和 ReturnListener(在 Spring AMQP 中是 ReturnCallback)。 1234567891011rabbitTemplate.setMandatory(true);rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback()&#123; public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey)&#123; System.out.println("回发的消息:"); System.out.println("replyCode: "+replyCode); System.out.println("replyText: "+replyText); System.out.println("exchange: "+exchange); System.out.println("routingKey: "+routingKey); &#125;&#125;); 消息路由到备份交换机的方式:在创建交换机的时候，从属性中指定备份交换机。 123Map&lt;String,Object&gt; arguments = new HashMap&lt;String,Object&gt;(); arguments.put("alternate-exchange","ALTERNATE_EXCHANGE"); // 指定交换机的备份交换机 ​channel.exchangeDeclare("TEST_EXCHANGE","topic", false, false, false, arguments); (注意区别，队列可以指定死信交换机;交换机可以指定备份交换机) 消息在 Queue 中存储第三个环节是消息在队列存储，如果没有消费者的话，队列一直存在在数据库中。 如果 RabbitMQ 的服务或者硬件发生故障，比如系统宕机、重启、关闭等等，可能 会导致内存中的消息丢失，所以我们要把消息本身和元数据(队列、交换机、绑定)都 保存到磁盘。 解决方案： 队列持久化12345@Bean("GpQueue")public Queue GpQueue() &#123; // queueName, durable, exclusive, autoDelete, Properties return new Queue("GP_TEST_QUEUE", true, false, false, new HashMap&lt;&gt;());&#125; 交换机持久化123456@Bean("GpExchange")public DirectExchange exchange() &#123; // exchangeName, durable, exclusive, autoDelete, Properties return new DirectExchange("GP_TEST_EXCHANGE", true, false, new HashMap&lt;&gt;());&#125; 消息持久化1234MessageProperties messageProperties = new MessageProperties();messageProperties.setDeliveryMode(MessageDeliveryMode.PERSISTENT);Message message = new Message("持久化消息".getBytes(), messageProperties);rabbitTemplate.send("GP_TEST_EXCHANGE", "gupao.test", message); 集群如果只有一个 RabbitMQ 的节点，即使交换机、队列、消息做了持久化，如果服务 崩溃或者硬件发生故障，RabbitMQ 的服务一样是不可用的，所以为了提高 MQ 服务的 可用性，保障消息的传输，我们需要有多个 RabbitMQ 的节点 消费者订阅 Queue 并消费消息如果消费者收到消息后没来得及处理即发生异常，或者处理过程中发生异常，会导 致4失败。服务端应该以某种方式得知消费者对消息的接收情况，并决定是否重新投递 这条消息给其他消费者。 RabbitMQ 提供了消费者的消息确认机制(message acknowledgement)，消费 者可以自动或者手动地发送 ACK 给服务端。 没有收到 ACK 的消息，消费者断开连接后，RabbitMQ 会把这条消息发送给其他消 费者。如果没有其他消费者，消费者重启后会重新消费这条消息，重复执行业务逻辑。 消费者在订阅队列时，可以指定 autoAck 参数，当 autoAck 等于 false 时，RabbitMQ 会等待消费者显式地回复确认信号后才从队列中移去消息。 如何设置手动 ACK? SimpleRabbitListenerContainer 或者 SimpleRabbitListenerContainerFactory 1factory.setAcknowledgeMode(AcknowledgeMode.MANUAL); application.properties 12spring.rabbitmq.listener.direct.acknowledge-mode=manual spring.rabbitmq.listener.simple.acknowledge-mode=manual 注意这三个值的区别: NONE:自动 ACKMANUAL: 手动 ACKAUTO:如果方法未抛出异常，则发送 ack。 当抛出 AmqpRejectAndDontRequeueException 异常的时候，则消息会被拒绝，且不重新入队。当抛出 ImmediateAcknowledgeAmqpException 异常，则消费者会 发送 ACK。其他的异常，则消息会被拒绝，且 requeue = true 会重新入队。 在springboot中：消费者又怎么调用 ACK，或者说怎么获得 Channel 参数呢? 1234567public class SecondConsumer &#123; @RabbitHandler public void process(String msgContent,Channel channel, Message message) throws IOException &#123; System.out.println("Second Queue received msg : " + msgContent ); channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125;&#125; 如果消息无法处理或者消费失败，也有两种拒绝的方式，Basic.Reject()拒绝单条， Basic.Nack()批量拒绝。如果 requeue 参数设置为 true，可以把这条消息重新存入队列， 以便发给下一个消费者(当然，只有一个消费者的时候，这种方式可能会出现无限循环 重复消费的情况。可以投递到新的队列中，或者只打印异常日志)。 思考:服务端收到了 ACK 或者 NACK，生产者会知道吗?即使消费者没有接收到消息，或者消费时出现异常，生产者也是完全不知情的。 例如，我们寄出去一个快递，是怎么知道收件人有没有收到的?因为有物流跟踪和 签收反馈，所以寄件人可以知道。 在没有用上电话的年代，我们寄出去一封信，是怎么知道收信人有没有收到信件? 只有收到回信，才知道寄出的信被收到了。 所以，这个是生产者最终确定消费者有没有消费成功的两种方式: 消费者收到消息，处理完毕后，调用生产者的API(思考:是否破坏解耦?) 消费者收到消息，处理完毕后，发送一条响应消息给生产者。 消费者回调1) 调用生产者 API 例如:提单系统给其他系统发送了碎屏保消息后，其他系统必须在处理完消息后调用提单系统提供的 API，来修改提单系统中数据的状态。只要 API 没有被调用， 数据状态没有被修改，提单系统就认为下游系统没有收到这条消息。 2) 发送响应消息给生产者 例如:商业银行与人民银行二代支付通信，无论是人行收到了商业银行的消息， 还是商业银行收到了人行的消息，都必须发送一条响应消息(叫做回执报文)。 补偿机制如果生产者的 API 就是没有被调用，也没有收到消费者的响应消息，怎么办? 不要着急，可能是消费者处理时间太长或者网络超时。 生产者与消费者之间应该约定一个超时时间，比如 5 分钟，对于超出这个时间没有得到响应的消息，可以设置一个定时重发的机制，但要发送间隔和控制次数，比如每隔 2 分钟发送一次，最多重发 3 次，否则会造成消息堆积。重发可以通过消息落库+定时任务来实现。重发，是否发送一模一样的消息? 参考:ATM 机上运行的系统叫 C 端(ATMC)，前置系统叫 P 端(ATMC)，它接收 ATMC的消息，再转发给卡系统或者核心系统。1)如果客户存款，没有收到核心系统的应答，不知道有没有记账成功，最多发次存款确认报文，因为已经吞钞了，所以要保证成功;2)如果客户取款，ATMC 未得到应答时，最多发送 5 次存款冲正报文。因为没有吐钞，所以要保证失败。 消息幂等性如果消费者每一次接收生产者的消息都成功了，只是在响应或者调用 API 的时候出 了问题，会不会出现消息的重复处理?例如:存款 100 元，ATM 重发了 5 次，核心系统 一共处理了 6 次，余额增加了 600 元。 所以，为了避免相同消息的重复处理，必须要采取一定的措施。RabbitMQ 服务端 是没有这种控制的(同一批的消息有个递增的 DeliveryTag)，它不知道你是不是就要把 一条消息发送两次，只能在消费端控制。 如何避免消息的重复消费? 消息出现重复可能会有两个原因: 生产者的问题，环节1重复发送消息，比如在开启了 Confirm 模式但未收到 确认，消费者重复投递。 环节4出了问题，由于消费者未发送 ACK 或者其他原因，消息重复投递。 生产者代码或者网络问题。 对于重复发送的消息，可以对每一条消息生成一个唯一的业务 ID，通过日志或者消息落库来做重复控制。 最终一致如果确实是消费者宕机了，或者代码出现了 BUG 导致无法正常消费，在我们尝试多 次重发以后，消息最终也没有得到处理，怎么办? 例如存款的场景，客户的钱已经被吞了，但是余额没有增加，这个时候银行出现了 长款，应该怎么处理?如果客户没有主动通知银行，这个问题是怎么发现的?银行最终 怎么把这个账务做平? 在我们的金融系统中，都会有双方对账或者多方对账的操作，通常是在一天的业务 结束之后，第二天营业之前。我们会约定一个标准，比如 ATM 跟核心系统对账，肯定是 以核心系统为准。ATMC 获取到核心的对账文件，然后解析，登记成数据，然后跟自己 记录的流水比较，找出核心有 ATM 没有，或者 ATM 有核心没有，或者两边都有但是金 额不一致的数据。 对账之后，我们再手工平账。比如取款记了账但是没吐钞的，做一笔冲正。存款吞 了钞没记账的，要么把钱退给客户，要么补一笔账。 消息的顺序性消息的顺序性指的是消费者消费消息的顺序跟生产者生产消息的顺序是一致的。例如:商户信息同步到其他系统，有三个业务操作:1、新增门店 2、绑定产品 3、 激活门店，这种情况下消息消费顺序不能颠倒(门店不存在时无法绑定产品和激活)。又比如:1、发表微博;2、发表评论;3、删除微博。顺序不能颠倒。在 RabbitMQ 中，一个队列有多个消费者时，由于不同的消费者消费消息的速度是不一样的，顺序无法保证。只有一个队列仅有一个消费者的情况才能保证顺序消费(不同的业务消息发送到不同的专用的队列)。 集群与高可用为什么要做集群集群主要用于实现高可用与负载均衡。 高可用:如果集群中的某些 MQ 服务器不可用，客户端还可以连接到其他 MQ 服务 器。 负载均衡:在高并发的场景下，单台 MQ 服务器能处理的消息有限，可以分发给多 台 MQ 服务器。 RabbitMQ 有两种集群模式:普通集群模式和镜像队列模式。 RabbitMQ 如何支持集群应用做集群，需要面对数据同步和通信的问题。因为 Erlang 天生具备分布式的特性， 所以 RabbitMQ 天然支持集群，不需要通过引入 ZK 或者数据库来实现数据同步。 RabbitMQ 通过/var/lib/rabbitmq/.erlang.cookie 来验证身份，需要在所有节点上 保持一致。 RabbitMQ 的节点类型集群有两种节点类型，一种是磁盘节点(Disc Node)，一种是内存节点(RAM Node)。磁盘节点:将元数据(包括队列名字属性、交换机的类型名字属性、绑定、vhost) 放在磁盘中。内存节点:将元数据放在内存中。PS:内存节点会将磁盘节点的地址存放在磁盘(不然重启后就没有办法同步数据了)。 如果是持久化的消息，会同时存放在内存和磁盘。集群中至少需要一个磁盘节点用来持久化元数据，否则全部内存节点崩溃时，就无 从同步元数据。未指定类型的情况下，默认为磁盘节点。我们一般把应用连接到内存节点(读写快)，磁盘节点用来备份。集群通过 25672 端口两两通信，需要开放防火墙的端口。需要注意的是，RabbitMQ 集群无法搭建在广域网上，除非使用 federation 或者 shovel 等插件(没这个必要，在同一个机房做集群)。 集群的配置步骤:1、配置 hosts2、同步 erlang.cookie3、加入集群(join cluster) 普通集群普通集群模式下，不同的节点之间只会相互同步元数据。 疑问:为什么不直接把队列的内容(消息)在所有节点上复制一份?主要是出于存储和同步数据的网络开销的考虑，如果所有节点都存储相同的数据， 就无法达到线性地增加性能和存储容量的目的(堆机器)。假如生产者连接的是节点 3，要将消息通过交换机 A 路由到队列 1，最终消息还是会 转发到节点 1 上存储，因为队列 1 的内容只在节点 1 上。同理，如果消费者连接是节点 2，要从队列 1 上拉取消息，消息会从节点 1 转发到 节点 2。其它节点起到一个路由的作用，类似于指针。 普通集群模式不能保证队列的高可用性，因为队列内容不会复制。如果节点失效将 导致相关队列不可用，因此我们需要第二种集群模式。 镜像集群第二种集群模式叫做镜像队列。镜像队列模式下，消息内容会在镜像节点间同步，可用性更高。不过也有一定的副 作用，系统性能会降低，节点过多的情况下同步的代价比较大。 高可用集群搭建成功后，如果有多个内存节点，那么生产者和消费者应该连接到哪个内存节点?如果在我们的代码中根据一定的策略来选择要使用的服务器，那每个地方都要修改，客户端的代码就会出现很多的重复，修改起来也比较麻烦。 所以需要一个负载均衡的组件(例如 HAProxy，LVS，Nignx)，由负载的组件来做路由。这个时候，只需要连接到负载组件的 IP 地址就可以了。 但是，如果这个负载的组件也挂了呢?客户端就无法连接到任意一台 MQ 的服务器 了。所以负载软件本身也需要做一个集群。新的问题又来了，如果有两台负载的软件， 客户端应该连哪个? 负载之上再负载?陷入死循环了。这个时候我们就要换个思路了。 我们应该需要这样一个组件:1、 它本身有路由(负载)功能，可以监控集群中节点的状态(比如监控 HAProxy)，如果某个节点出现异常或者发生故障，就把它剔除掉。2、 为了提高可用性，它也可以部署多个服务，但是只有一个自动选举出 来的 MASTER 服务器(叫做主路由器)，通过广播心跳消息实现。3、 MASTER 服务器对外提供一个虚拟 IP，提供各种网络功能。也就是 谁抢占到 VIP，就由谁对外提供网络服务。应用端只需要连接到这一 个 IP 就行了。 这个协议叫做 VRRP 协议(虚拟路由冗余协议 Virtual Router Redundancy Protocol)，这个组件就是 Keepalived，它具有 Load Balance 和 High Availability 的功能。 下面我们看下用 HAProxy 和 Keepalived 如何实现 RabbitMQ 的高可用 (MySQL、Mycat、Redis 类似)。 基于 Docker 安装 HAproxy 负载+Keepalived 高可用 规划:内存节点 1:192.168.8.40 内存节点 2:192.168.8.45 磁盘节点:192.168.8.150 VIP:192.168.8.220 1、我们规划了两个内存节点，一个磁盘节点。所有的节点之间通过镜像队列的 方式同步数据。内存节点用来给应用访问，磁盘节点用来持久化数据。2、为了实现对两个内存节点的负载，我们安装了两个 HAProxy，监听两个 5672 和 15672 的端口。3、安装两个 Keepalived，一主一备。两个 Keepalived 抢占一个 VIP192.168.8.220。谁抢占到这个 VIP，应用就连接到谁，来执行对 MQ 的负载。这种情况下，我们的 Keepalived 挂了一个节点，没有影响，因为 BACKUP 会变 成 MASTER，抢占 VIP。HAProxy 挂了一个节点，没有影响，我们的 VIP 会自动路 由的可用的 HAProxy 服务。RabbitMQ 挂了一个节点，没有影响， 因为 HAProxy 会自动负载到可用的节点。 实践经验总结资源管理到底在消费者创建还是在生产者创建?如果 A 项目和 B 项目有相互发送和接收消息，应该创建几个 vhost，几个 Exchange?交换机和队列，实际上是作为资源，由运维管理员创建的。 为什么仍然需要在代码中定义?重复创建不报错吗? 配置文件与命名规范1、元数据的命名集中放在 properties 文件中，不要用硬编码。如果有多个系统， 可以配置多个 xxx_mq.properties。 2、命名体现元数据的类型 虚拟机命名: XXX_VHOST 交换机命名:XXX_EXCHANGE 队列命名:_QUEUE 3、命名体现数据来源和去向例如:销售系统发往产品系统的交换机:SALE_TO_PRODUCT_EXCHANGE。做到见名知义，不用去查文档(当然注释是必不可少的)。 调用封装在项目中可以对 Template 做进一步封装，简化消息的发送。 例如:如果交换机、路由键是固定的，封装之后就只需要一个参数:消息内容。 另外，如果想要平滑地迁移不同的 MQ(如果有这种需求的话)，也可以再做一层简单的封装。 123456789GpSendMsg()&#123; JmsTemplate.send(destination,msg); &#125;这时，如果要把 ActiveMQ 替换为 RabbitMQ，只需要修改:GpSendMsg()&#123; RabbitTemplate.send(exchange,routingKey,msg); &#125; 信息落库+定时任务将需要发送的消息保存在数据库中，可以实现消息的可追溯和重复控制，需要配合定时任务来实现。1) 将需要发送的消息登记在消息表中。2) 定时任务一分钟或半分钟扫描一次，将未发送的消息发送到 MQ 服务器，并且修改状态为已发送。3) 如果需要重发消息，将指定消息的状态修改为未发送即可。 生产环境运维监控虽然 RabbitMQ 提供了一个简单的管理界面，但是如果对于系统性能、高可用和其他参数有一些定制化的监控需求的话，我们就需要通过其他方式来实现监控了。主要关注:磁盘、内存、连接数。zabbix 日志追踪RabbitMQ 可以通过 Firehose 功能来记录消息流入流出的情况，用于调试，排错。 它是通过创建一个 TOPIC 类型的交换机(amq.rabbitmq.trace)，把生产者发送给 Broker 的消息或者 Broker 发送给消费者的消息发到这个默认的交换机上面来实现的。 另外 RabbitMQ 也提供了一个 Firehose 的 GUI 版本，就是 Tracing 插件。 启用 Tracing 插件后管理界面右侧选项卡会多一个 Tracing，可以添加相应的策略。 RabbitMQ 还提供了其他的插件来增强功能。https://www.rabbitmq.com/firehose.html 如何减少连接数在发送大批量消息的情况下，创建和释放连接依然有不小的开销。我们可以跟接收方约定批量消息的格式，比如支持 JSON 数组的格式，通过合并消息内容，可以减少生产者/消费者与 Broker 的连接。 比如:活动过后，要全范围下线产品，通过 Excel 导入模板，通常有几万到几十万条 解绑数据，合并发送的效率更高。 建议单条消息不要超过 4M(4096KB)，一次发送的消息数需要合理地控制。 面试题1、 消息队列的作用与使用场景? 2、 Channel 和 vhost 的作用是什么?Channel:减少 TCP 资源的消耗。也是最重要的编程接口。 Vhost:提高硬件资源利用率，实现资源隔离。 3、 RabbitMQ 的消息有哪些路由方式?适合在什么业务场景使用?Direct、Topic、Fanout 4、 交换机与队列、队列与消费者的绑定关系是什么样的? 多个消费者监听一个队列时(比如一个服务部署多个实例)，消息会重复消费吗?多对多;轮询(平均分发) 5、 无法被路由的消息，去了哪里? 直接丢弃。可用备份交换机(alternate-exchange)接收。 6、 消息在什么时候会变成 Dead Letter(死信)? 消息过期;消息超过队列长度或容量;消息被拒绝并且未设置重回队列 7、 如果一个项目要从多个服务器接收消息，怎么做? 如果一个项目要发送消息到多个服务器，怎么做?定义多个 ConnectionFactory，注入到消费者监听类/Temaplate。 8、 RabbitMQ 如何实现延迟队列? 基于数据库+定时任务; 或者消息过期+死信队列; 或者延迟队列插件。 9、 哪些情况会导致消息丢失?怎么解决? 哪些情况会导致消息重复?怎么解决? 从消息发送的整个流程来分析。 10、 一个队列最多可以存放多少条消息? 由硬件决定。 11、 可以用队列的 x-max-length 最大消息数来实现限流吗?例如秒杀场景。 不能，因为会删除先入队的消息，不公平。 12、 如何提高消息的消费速率? 创建多个消费者。 13、 AmqpTemplate 和 RabbitTemplate 的区别?Spring AMQP 是 Spring 整合 AMQP 的一个抽象。Spring-Rabbit 是一个实现。 14、 如何动态地创建消费者监听队列? 通过 ListenerContainer 1container.setQueues(getSecondQueue(), getThirdQueue()); //监听的队列 15、 Spring AMQP 中消息怎么封装?用什么转换? Message，MessageConvertor 16、 如何保证消息的顺序性? 一个队列只有一个消费者 17、 RabbitMQ 的集群节点类型? 磁盘节点和内存节点 18、 如何保证 RabbitMQ 的高可用?HAProxy(LVS)+Keepalived 19、 大量消息堆积怎么办?1) 重启(不是开玩笑的)2) 多创建几个消费者同时消费3) 直接清空队列，重发消息]]></content>
      <tags>
        <tag>mq rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2020%2F03%2F18%2Fredis%2F</url>
    <content type="text"><![CDATA[redis 的基础数据类型redis面试题reds数据如何实例化]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql_索引]]></title>
    <url>%2F2020%2F03%2F18%2Fmysql_%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引是什么索引图解数据库索引，是数据库管理系统(DBMS)中一个排序的数据结构，以协助快速查询、 更新数据库表中数据。 首先数据是以文件的形式存放在磁盘上面的，每一行数据都有它的磁盘地址。如果 没有索引的话，要从 500 万行数据里面检索一条数据，只能依次遍历这张表的全部数据， 直到找到这条数据。 但是有了索引之后，只需要在索引里面去检索这条数据就行了，因为它是一种特殊 的专门用来快速检索的数据结构，我们找到数据存放的磁盘地址以后，就可以拿到数据 了。 就像我们从一本 500 页的书里面去找特定的一小节的内容，肯定不可能从第一页开 始翻。那么这本书有专门的目录，它可能只有几页的内容，它是按页码来组织的，可以 根据拼音或者偏旁部首来查找，只要确定内容对应的页码，就能很快地找到我们想要的 内容。 索引类型在 InnoDB 里面，索引类型有三种，普通索引、唯一索引(主键索引是特殊的唯一 索引)、全文索引。 普通(Normal):也叫非唯一索引，是最普通的索引，没有任何的限制。 唯一(Unique):唯一索引要求键值不能重复。另外需要注意的是，主键索引是一 种特殊的唯一索引，它还多了一个限制条件，要求键值不能为空。主键索引用 primay key 创建。 全文(Fulltext):针对比较大的数据，比如我们存放的是消息内容，有几 KB 的数 据的这种情况，如果要解决 like 查询效率低的问题，可以创建全文索引。只有文本类型 的字段才可以创建全文索引，比如 char、varchar、text。 索引的存储类型模型推演二分查找双十一过去之后，你女朋友跟你玩了一个猜数字的游戏。 猜猜我昨天买了多少钱，给你五次机会。 10000?低了。30000?高了。接下来你会猜多少? 20000。为什么你不猜 11000，也不猜 29000 呢? 其实这个就是二分查找的一种思想，也叫折半查找，每一次，我们都把候选数据缩小了一半。如果数据已经排过序的话，这种方式效率比较高。 所以第一个，我们可以考虑用有序数组作为索引的数据结构。 有序数组的等值查询和比较查询效率非常高，但是更新数据的时候会出现一个问题，可能要挪动大量的数据(改变 index)，所以只适合存储静态的数据。 为了支持频繁的修改，比如插入数据，我们需要采用链表。链表的话，如果是单链表，它的查找效率还是不够高。所以，有没有可以使用二分查找的链表呢? 为了解决这个问题，BST(Binary Search Tree)也就是我们所说的二叉查找树诞生了。 二叉查找树(BST Binary Search Tree)二叉查找树的特点是什么? 左子树所有的节点都小于父节点，右子树所有的节点都大于父节点。投影到平面以 后，就是一个有序的线性表。 二叉查找树既能够实现快速查找，又能够实现快速插入。 但是二叉查找树有一个问题: 就是它的查找耗时是和这棵树的深度相关的，在最坏的情况下时间复杂度会退化成O(n)。 什么情况是最坏的情况呢?我们打开这样一个网站来看一下，这里面有各种各样的数据结构的动态演示，包括 BST 二叉查找树:https://www.cs.usfca.edu/~galles/visualization/Algorithms.html 如果插入的数据刚好是有序的，1，2，3，4，5 这个时候我们的二叉查找树变成了什么样了呢? 它会变成链表(我们把这种树叫做“斜树”)，这种情况下不能达到加快检索速度 的目的，和顺序查找效率是没有区别的。 造成它倾斜的原因是什么呢? 因为左右子树深度差太大，这棵树的左子树根本没有节点——也就是它不够平衡。 平衡二叉树(AVL Tree)(左旋、右旋)AVL Trees (Balanced binary search trees) 平衡二叉树的定义:左右子树深度差绝对值不能超过 1。 是什么意思呢?比如左子树的深度是 2，右子树的深度只能是 1 或者 3。 这个时候我们再按顺序插入 1、2、3、4、5、6，一定是这样，不会变成一棵“斜树”。 插入 1、2、3。我们注意看:当我们插入了 1、2 之后，如果按照二叉查找树的定义，3 肯定是要在 2 的右边的，这个时候根节点 1 的右节点深度会变成 2，但是左节点的深度是 0，因为它 没有子节点，所以就会违反平衡二叉树的定义。 那应该怎么办呢?因为它是右节点下面接一个右节点，右-右型，所以这个时候我们 要把 2 提上去，这个操作叫做左旋。 右旋类似所以为了保持平衡，AVL 树在插入和更新数据的时候执行了一系列的计算和调整的 操作。 平衡的问题我们解决了，那么平衡二叉树作为索引怎么查询数据? 在平衡二叉树中，一个节点，它的大小是一个固定的单位，作为索引应该存储什么 内容? 它应该存储三块的内容: 第一个是索引的键值。比如我们在 id 上面创建了一个索引，我在用 where id =1 的 条件查询的时候就会找到索引里面的 id 的这个键值。 第二个是数据的磁盘地址，因为索引的作用就是去查找数据的存放的地址。 第三个，因为是二叉树，它必须还要有左子节点和右子节点的引用，这样我们才能 找到下一个节点。比如大于 26 的时候，走右边，到下一个树的节点，继续判断。 InnoDB 逻辑存储结构MySQL 的存储结构分为 5 级:表空间、段、簇、页、行。 表空间 Table Space表空间可以看做是 InnoDB 存储引擎逻辑结构的 最高层，所有的数据都存放在表空间中。分为:系统表空间、独占表空间、通用表空间、 临时表空间、Undo 表空间。 段 Segment表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等，段是一个逻辑 的概念。一个 ibd 文件(独立表空间文件)里面会由很多个段组成。 创建一个索引会创建两个段，一个是索引段:leaf node segment，一个是数据段: non-leaf node segment。索引段管理非叶子节点的数据。数据段管理叶子节点的数据。 也就是说，一个表的段数，就是索引的个数乘以 2。 簇 Extent一个段(Segment)又由很多的簇(也可以叫区)组成，每个区的大小是 1MB(64 个连续的页)。 每一个段至少会有一个簇，一个段所管理的空间大小是无限的，可以一直扩展下去， 但是扩展的最小单位就是簇。 页 Page为了高效管理物理空间，对簇进一步细分，就得到了页。簇是由连续的页(Page) 组成的空间，一个簇中有 64 个连续的页。 (1MB/16KB=64)。这些页面在物理上和 逻辑上都是连续的。 跟大多数数据库一样，InnoDB 也有页的概念(也可以称为块)，每个页默认 16KB。 页是 InnoDB 存储引擎磁盘管理的最小单位，通过 innodb_page_size 设置。 一个表空间最多拥有 2^32 个页，默认情况下一个页的大小为 16KB，也就是说一个 表空间最多存储 64TB 的数据。 注意，文件系统中，也有页的概念。操作系统和内存打交道，最小的单位是页 Page。文件系统的内存页通常是 4K。 1SHOW VARIABLES LIKE 'innodb_page_size'; 假设一行数据大小是 1K，那么一个数据页可以放 16 行这样的数据。 举例:一个页放 3 行数据。 往表中插入数据时，如果一个页面已经写完，产生一个新的叶页面。如果一个簇的 所有的页面都被用完，会从当前页面所在段新分配一个簇。 如果数据不是连续的，往已经写满的页中插入数据，会导致叶页面分裂: 连续 不连续 行 RowInnoDB 存储引擎是面向行的(row-oriented)，也就是说数据的存放按行进行存放。 AVL 树用于存储索引数据首先，索引的数据，是放在硬盘上的。查看数据和索引的大小: 1234selectCONCAT(ROUND(SUM(DATA_LENGTH/1024/1024),2),'MB') AS data_len,CONCAT(ROUND(SUM(INDEX_LENGTH/1024/1024),2),'MB') as index_len from information_schema.TABLESwhere table_schema='test' and table_name='account'; 当我们用树的结构来存储索引的时候，访问一个节点就要跟磁盘之间发生一次 IO。 InnoDB 操作磁盘的最小的单位是一页(或者叫一个磁盘块)，大小是 16K(16384 字节)。 那么，一个树的节点就是 16K 的大小。 如果我们一个节点只存一个键值+数据+引用，例如整形的字段，可能只用了十几个 或者几十个字节，它远远达不到 16K 的容量，所以访问一个树节点，进行一次 IO 的时候， 浪费了大量的空间。 所以如果每个节点存储的数据太少，从索引中找到我们需要的数据，就要访问更多 的节点，意味着跟磁盘交互次数就会过多。 如果是机械硬盘时代，每次从磁盘读取数据需要 10ms 左右的寻址时间，交互次数 越多，消耗的时间就越多。 比如上面这张图，我们一张表里面有 6 条数据，当我们查询 id=37 的时候，要查询两个子节点，就需要跟磁盘交互 3 次，如果我们有几百万的数据呢?这个时间更加难以估计。 所以我们的解决方案是什么呢?第一个就是让每个节点存储更多的数据。第二个，节点上的关键字的数量越多，我们的指针数也越多，也就是意味着可以有更多的分叉(我们把它叫做“路数”)。因为分叉数越多，树的深度就会减少(根节点是 0)。这样，我们的树是不是从原来的高瘦高瘦的样子，变成了矮胖矮胖的样子?这个时候，我们的树就不再是二叉了，而是多叉，或者叫做多路。 多路平衡查找树(B Tree)(分裂、合并)Balanced Tree这个就是我们的多路平衡查找树，叫做 B Tree(B 代表平衡)。跟 AVL 树一样，B 树在枝节点和叶子节点存储键值、数据地址、节点引用。 它有一个特点:分叉数(路数)永远比关键字数多 1。比如我们画的这棵树，每个节点存储两个关键字，那么就会有三个指针指向三个子节点。B Tree 的查找规则是什么样的呢? 比如我们要在这张表里面查找 15。因为 15 小于 17，走左边。因为 15 大于 12，走右边。在磁盘块 7 里面就找到了 15，只用了 3 次 IO。 那 B Tree 又是怎么实现一个节点存储多个关键字，还保持平衡的呢?跟 AVL 树有什 么区别? https://www.cs.usfca.edu/~galles/visualization/Algorithms.html 比如 Max Degree(路数)是 3 的时候，我们插入数据 1、2、3，在插入 3 的时候， 本来应该在第一个磁盘块，但是如果一个节点有三个关键字的时候，意味着有 4 个指针， 子节点会变成 4 路，所以这个时候必须进行分裂。把中间的数据 2 提上去，把 1 和 3 变 成 2 的子节点。 如果删除节点，会有相反的合并的操作。 注意这里是分裂和合并，跟 AVL 树的左旋和右旋是不一样的。 我们继续插入 4 和 5，B Tree 又会出现分裂和合并的操作。 从这个里面我们也能看到，在更新索引的时候会有大量的索引的结构的调整，所以 解释了为什么我们不要在频繁更新的列上建索引，或者为什么不要更新主键。 节点的分裂和合并，其实就是 InnoDB 页的分裂和合并。 B+树(加强版多路平衡查找树)B Tree 的效率已经很高了，为什么 MySQL 还要对 B Tree 进行改良，最终使用了 B+Tree 呢? 总体上来说，这个 B 树的改良版本解决的问题比 B Tree 更全面。 B+树的存储结构: MySQL 中的 B+Tree 有几个特点: 它的关键字的数量是跟路数相等的; B+Tree 的根节点和枝节点中都不会存储数据，只有叶子节点才存储数据。搜索到关键字不会直接返回，会到最后一层的叶子节点。比如我们搜索 id=28，虽然在第一 层直接命中了，但是全部的数据在叶子节点上面，所以我还要继续往下搜索，一直到叶子节点。 举个例子:假设一条记录是 1K，一个叶子节点(一页)可以存储 16 条记录。非叶 子节点可以存储多少个指针?假设索引字段是 bigint 类型，长度为 8 字节。指针大小在 InnoDB 源码中设置为 6 字节，这样一共 14 字节。非叶子节点(一页)可以存储 16384/14=1170 个这样的 单元(键值+指针)，代表有 1170 个指针。 树深度为 2 的时候，有 1170^2 个叶子节点，可以存储的数据为 1170117016=21902400。 在查找数据时一次页的查找代表一次 IO，也就是说，一张 2000 万左右的表，查询 数据最多需要访问 3 次磁盘。所以在 InnoDB 中 B+ 树深度一般为 1-3 层，它就能满足千万级的数据存储。 B+Tree 的每个叶子节点增加了一个指向相邻叶子节点的指针，它的最后一个数 据会指向下一个叶子节点的第一个数据，形成了一个有序链表的结构。 它是根据左闭右开的区间 [ )来检索数据。 B+Tree 的数据搜寻过程: 比如我们要查找 28，在根节点就找到了键值，但是因为它不是页子节点，所以 会继续往下搜寻，28 是[28,66)的左闭右开的区间的临界值，所以会走中间的子节点，然 后继续搜索，它又是[28,34)的左闭右开的区间的临界值，所以会走左边的子节点，最后 在叶子节点上找到了需要的数据。 第二个，如果是范围查询，比如要查询从 22 到 60 的数据，当找到 22 之后，只 需要顺着节点和指针顺序遍历就可以一次性访问到所有的数据节点，这样就极大地提高了区间查询效率(不需要返回上层父节点重复遍历查找)。 InnoDB 中的 B+Tree 的特点: 它是 B Tree 的变种，B Tree 能解决的问题，它都能解决。B Tree 解决的两大问题 是什么?(每个节点存储更多关键字;路数更多) 扫库、扫表能力更强(如果我们要对表进行全表扫描，只需要遍历叶子节点就可以 了，不需要遍历整棵 B+Tree 拿到所有的数据) B+Tree 的磁盘读写能力相对于 B Tree 来说更强(根节点和枝节点不保存数据区， 所以一个节点可以保存更多的关键字，一次磁盘加载的关键字更多) 排序能力更强(因为叶子节点上有下一个数据区的指针，数据形成了链表) 效率更加稳定(B+Tree 永远是在叶子节点拿到数据，所以 IO 次数是稳定的) 为什么不用红黑树?红黑树也是 BST 树，但是不是严格平衡的。 红黑树必须满足 5 个约束: 节点分为红色或者黑色。 根节点必须是黑色的。 叶子节点都是黑色的 NULL 节点。 红色节点的两个子节点都是黑色(不允许两个相邻的红色节点)。 从任意节点出发，到其每个叶子节点的路径中包含相同数量的黑色节点。 插入:60、56、68、45、64、58、72、43、49 基于以上规则，可以推导出: 从根节点到叶子节点的最长路径(红黑相间的路径)不大于最短路径(全部是黑色 节点)的 2 倍。 为什么不用红黑树?1、只有两路;2、不够平衡。 红黑树一般只放在内存里面用。例如 Java 的 TreeMap。 索引方式:真的是用的 B+Tree 吗?在 Navicat 的工具中，创建索引，索引方式有两种，Hash 和 B Tree。 HASH:以 KV 的形式检索数据，也就是说，它会根据索引字段生成哈希码和指针， 指针指向数据。 哈希索引有什么特点呢? 第一个，它的时间复杂度是 O(1)，查询速度比较快。因为哈希索引里面的数据不是 按顺序存储的，所以不能用于排序。 第二个，我们在查询数据的时候要根据键值计算哈希码，所以它只能支持等值查询 (= IN)，不支持范围查询(&gt; &lt; &gt;= &lt;= between and)。 另外一个就是如果字段重复值很多的时候，会出现大量的哈希冲突(采用拉链法解 决)，效率会降低。 问题: InnoDB 可以在客户端创建一个索引，使用哈希索引吗?https://dev.mysql.com/doc/refman/5.7/en/innodb-introduction.htmlInnoDB utilizes hash indexes internally for its Adaptive Hash Index feature 直接翻译过来就是:InnoDB 内部使用哈希索引来实现自适应哈希索引特性。这句话的意思是 InnoDB 只支持显式创建 B+Tree 索引，对于一些热点数据页， InnoDB 会自动建立自适应 Hash 索引，也就是在 B+Tree 索引基础上建立 Hash 索引， 这个过程对于客户端是不可控制的，隐式的。 我们在 Navicat 工具里面选择索引方法是哈希，但是它创建的还是 B+Tree 索引，这 个不是我们可以手动控制的。 B+tree 落地形式MySQL 架构MySQL 是一个支持插件式存储引擎的数据库。在 MySQL 里 面，每个表在创建的时候都可以指定它所使用的存储引擎。 这里我们主要关注一下最常用的两个存储引擎，MyISAM 和 InnoDB 的索引的实现。 MySQL 数据存储文件首先，MySQL 的数据都是文件的形式存放在磁盘中的，我们可以找到这个数据目录 的地址。在 MySQL 中有这么一个参数，我们来看一下: 1show VARIABLES LIKE 'datadir'; 每个数据库有一个目录，我们新建了一个叫做 gupao 的数据库，那么这里就有一个 gupao 的文件夹。这个数据库里面我们又建了 5 张表:archive、innodb、memory、myisam、csv。我们进入 gupao 的目录，发现这里面有一些跟我们创建的表名对应的文件。 在这里我们能看到，每张 InnoDB 的表有两个文件(.frm 和.ibd)，MyISAM 的表 有三个文件(.frm、.MYD、.MYI)。 有一个是相同的文件，.frm。 .frm 是 MySQL 里面表结构定义的文件，不管你建表 的时候选用任何一个存储引擎都会生成。 我们主要看一下其他两个文件是怎么实现 MySQL 不同的存储引擎的索引的。 MyISAM在 MyISAM 里面，另外有两个文件: 一个是.MYD 文件，D 代表 Data，是 MyISAM 的数据文件，存放数据记录，比如我 们的 user_myisam 表的所有的表数据。 一个是.MYI 文件，I 代表 Index，是 MyISAM 的索引文件，存放索引，比如我们在 id 字段上面创建了一个主键索引，那么主键索引就是在这个索引文件里面。 也就是说，在 MyISAM 里面，索引和数据是两个独立的文件。 那我们怎么根据索引找到数据呢?MyISAM 的 B+Tree 里面，叶子节点存储的是数据文件对应的磁盘地址。所以从索引文件.MYI 中找到键值后，会到数据文件.MYD 中获取相应的数据记录。 这里是主键索引，如果是辅助索引，有什么不一样呢? 在 MyISAM 里面，辅助索引也在这个.MYI 文件里面。 辅助索引跟主键索引存储和检索数据的方式是没有任何区别的，一样是在索引文件里面找到磁盘地址，然后到数据文件里面获取数据。 InnodbInnoDB 只有一个文件(.ibd 文件)，那索引放在哪里呢?在 InnoDB 里面，它是以主键为索引来组织数据的存储的，所以索引文件和数据文 件是同一个文件，都在.ibd 文件里面。在 InnoDB 的主键索引的叶子节点上，它直接存储了我们的数据。 什么叫做聚集索引(聚簇索引)?就是索引键值的逻辑顺序跟表数据行的物理存储顺序是一致的。(比如字典的目录 是按拼音排序的，内容也是按拼音排序的，按拼音排序的这种目录就叫聚集索引)。在 InnoDB 里面，它组织数据的方式叫做叫做(聚集)索引组织表(clustered index organize table)，所以主键索引是聚集索引，非主键都是非聚集索引。如果 InnoDB 里面主键是这样存储的，那主键之外的索引，比如我们在 name 字段 上面建的普通索引，又是怎么存储和检索数据的呢? InnoDB 中，主键索引和辅助索引是有一个主次之分的。辅助索引存储的是辅助索引和主键值。如果使用辅助索引查询，会根据主键值在主 键索引中查询，最终取得数据。比如我们用 name 索引查询 name= ‘青山’，它会在叶子节点找到主键值，也就是 id=1，然后再到主键索引的叶子节点拿到数据。 为什么在辅助索引里面存储的是主键值而不是主键的磁盘地址呢?如果主键的数据 类型比较大，是不是比存地址更消耗空间呢?我们前面说到 B Tree 是怎么实现一个节点存储多个关键字，还保持平衡的呢?是因为有分叉和合并的操作，这个时候键值的地址会发生变化，所以在辅助索引里 面不能存储地址。 另一个问题，如果一张表没有主键怎么办?1、如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。2、如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索引作为主键索引。3、如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐藏的聚集索引，它会随着行记录的写入而主键递增。 1select _rowid name from t2; 索引的使用原则列的离散度第一个叫做列的离散度，我们先来看一下列的离散度的公式:count(distinct(column_name)) : count(*)，列的全部不同值和所有数据行的比例。 数据行数相同的情况下，分子越大，列的离散度就越高。简单来说，如果列的重复值越多，离散度就越低，重复值越少，离散度就越高。 如果在 B+Tree 里面的重复值太多，MySQL 的优化器发现走索引跟使用全表扫描差不了多少的时候，就算建了索引，也不一定会走索引。 联合索引最左匹配前面我们说的都是针对单列创建的索引，但有的时候我们的多条件查询的时候，也 会建立联合索引。单列索引可以看成是特殊的联合索引。比如我们在 user 表上面，给 name 和 phone 建立了一个联合索引。联合索引在 B+Tree 中是复合的数据结构，它是按照从左到右的顺序来建立搜索树。(name 在左边，phone 在右边)。 从这张图可以看出来，name 是有序的，phone 是无序的。当 name 相等的时候， phone 才是有序的。 这个时候我们使用 where name= ‘青山’ and phone = ‘136xx ‘去查询数据的时候， B+Tree 会优先比较 name 来确定下一步应该搜索的方向，往左还是往右。如果 name 相同的时候再比较 phone。但是如果查询条件没有 name，就不知道第一步应该查哪个 节点，因为建立搜索树的时候 name 是第一个比较因子，所以用不到索引。 如果我们创建三个字段的索引 index(a,b,c)，相当于创建三个索引:index(a)index(a,b)index(a,b,c)用 where b=? 和 where b=? and c=? 和 where a=? and c=?是不能使用到索引的。不能不用第一个字段，不能中断。 覆盖索引回表:非主键索引，我们先通过索引找到主键索引的键值，再通过主键值查出索引里面没 有的数据，它比基于主键索引的查询多扫描了一棵索引树，这个过程就叫回表。例如:select * from user_innodb where name = ‘Qingshan’; 在辅助索引里面，不管是单列索引还是联合索引，如果 select 的数据列只用从索引 中就能够取得，不必从数据区中读取，这时候使用的索引就叫做覆盖索引，这样就避免了回表。 我们先来创建一个联合索引: 123-- 创建联合索引ALTER TABLE user_innodb DROP INDEX comixd_name_phone;ALTER TABLE user_innodb add INDEX `comixd_name_phone` (`name`,`phone`); 这三个查询语句都用到了覆盖索引: 123-- 创建联合索引EXPLAIN SELECT name,phone FROM user_innodb WHERE name= '青山' AND phone = '13666666666'; EXPLAIN SELECT name FROM user_innodb WHERE name= '青山' AND phone = ' 13666666666'; EXPLAIN SELECT phone FROM user_innodb WHERE name= '青山' AND phone = ' 13666666666'; Extra 里面值为“Using index”代表使用了覆盖索引。 select * ，用不到覆盖索引。 很明显，因为覆盖索引减少了 IO 次数，减少了数据的访问量，可以大大地提升查询 效率。 索引条件下推(ICP)再来看这么一张表，在 last_name 和 first_name 上面创建联合索引。 12345678910111213141516171819202122drop table employees;CREATE TABLE `employees` (`emp_no` int(11) NOT NULL, `birth_date` date NULL,`first_name` varchar(14) NOT NULL,`last_name` varchar(16) NOT NULL,`gender` enum('M','F') NOT NULL,`hire_date` date NULL,PRIMARY KEY (`emp_no`)) ENGINE=InnoDB DEFAULT CHARSET=latin1;alter table employees add index idx_lastname_firstname(last_name,first_name); ​INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '698', 'liu', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, 'd99', 'zheng', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, 'e08', 'huang', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '59d', 'lu', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '0dc', 'yu', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '989', 'wang', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, 'e38', 'wang', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '0zi', 'wang', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, 'dc9', 'xie', 'F', NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, '5ba', 'zhou', 'F', NULL); 关闭 ICP: 1set optimizer_switch='index_condition_pushdown=off'; 查看参数 1show variables like 'optimizer_switch'; 现在我们要查询所有姓 wang，并且名字最后一个字是 zi 的员工，比如王胖子，王 瘦子。查询的 SQL: 1select * from employees where last_name='wang' and first_name LIKE '%zi' ; 这条 SQL 有两种执行方式:1、根据联合索引查出所有姓 wang 的二级索引数据，然后回表，到主键索引上查询 全部符合条件的数据(3 条数据)。然后返回给 Server 层，在 Server 层过滤出名字以 zi 结尾的员工。2、根据联合索引查出所有姓 wang 的二级索引数据(3 个索引)，然后从二级索引 中筛选出 first_name 以 zi 结尾的索引(1 个索引)，然后再回表，到主键索引上查询全 部符合条件的数据(1 条数据)，返回给 Server 层。 很明显，第二种方式到主键索引上查询的数据更少。注意，索引的比较是在存储引擎进行的，数据记录的比较，是在 Server 层进行的。 而当 first_name 的条件不能用于索引过滤时，Server 层不会把 first_name 的条件传递 给存储引擎，所以读取了两条没有必要的记录。这时候，如果满足 last_name=’wang’的记录有 100000 条，就会有 99999 条没有 必要读取的记录。 执行以下 SQL，Using where: 1explain select * from employees where last_name='wang' and first_name LIKE '%zi' ; Using Where 代表从存储引擎取回的数据不全部满足条件，需要在 Server 层过滤。先用 last_name 条件进行索引范围扫描，读取数据表记录，然后进行比较，检查是 否符合 first_name LIKE ‘%zi’ 的条件。此时 3 条中只有 1 条符合条件。 开启 ICP: 1set optimizer_switch='index_condition_pushdown=on'; 此时的执行计划，Using index condition:把 first_name LIKE ‘%zi’下推给存储引擎后，只会从数据表读取所需的 1 条记录。 索引条件下推(Index Condition Pushdown)，5.6 以后完善的功能。只适用于二级索引。ICP 的目标是减少访问表的完整行的读数量从而减少 I/O 操作。 前缀索引当字段值比较长的时候，建立索引会消耗很多的空间，搜索起来也会很慢。我们可以通过截取字段的前面一部分内容建立索引，这个就叫前缀索引。创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引: 12create table shop(address varchar(120) not null); alter table shop add key (address(12)); 问题是，截取多少呢?截取得多了，达不到节省索引存储空间的目的，截取得少了， 重复内容太多，字段的散列度(选择性)会降低。怎么计算不同的长度的选择性呢?先看一下字段在全部数据中的选择度: 12345678select count(distinct address) / count(*) from shop;select count(distinct left(address,10))/count(*) as sub10,count(distinct left(address,11))/count(*) as sub11,count(distinct left(address,12))/count(*) as sub12,count(distinct left(address,13))/count(*) as sub13from shop; 只要截取前 13 个字段，就已经有比较高的选择性了(这里的数据只是举例)。 索引的创建与使用因为索引对于改善查询性能的作用是巨大的，所以我们的目标是尽量使用索引。 索引的创建1、在用于 where 判断 order 排序和 join 的(on)字段上创建索引。2、索引的个数不要过多。——浪费空间，更新变慢。3、区分度低的字段，例如性别，不要建索引。 ——离散度太低，导致扫描行数过多。4、频繁更新的值，不要作为主键或者索引。——页分裂5、组合索引把散列性高(区分度高)的值放在前面。6、创建复合索引，而不是修改单列索引。7、过长的字段，怎么建立索引?8、为什么不建议用无序的值(例如身份证、UUID )作为索引? 什么时候用不到索引?1、索引列上使用函数(replace\SUBSTR\CONCAT\sum count avg)、表达式、 计算(+ - * /): 1explain SELECT * FROM `t2` where id+1 = 4; 2、字符串不加引号，出现隐式转换 12345ALTER TABLE user_innodb DROP INDEX comidx_name_phone;ALTER TABLE user_innodb add INDEX comidx_name_phone (name,phone);explain SELECT * FROM `user_innodb` where name = 136; explain SELECT * FROM `user_innodb` where name = '136'; 3、like 条件中前面带%where 条件中 like abc%，like %2673%，like %888 都用不到索引吗?为什么? 12explain select *from user_innodb where name like 'wang%';explain select *from user_innodb where name like '%wang'; 过滤的开销太大，所以无法使用索引。这个时候可以用全文索引。4、负向查询NOT LIKE 不能: 1explain select *from employees where last_name not like 'wang' != (&lt;&gt;)和 NOT IN 在某些情况下可以: 12explain select *from employees where emp_no not in (1);explain select *from employees where emp_no &lt;&gt; 1; 注意一个 SQL 语句是否使用索引，跟数据库版本、数据量、数据选择度都有关系。其实，用不用索引，最终都是优化器说了算。优化器是基于什么的优化器?基于 cost 开销(Cost Base Optimizer)，它不是基于规则(Rule-Based Optimizer)，也不是基于语义。怎么样开销小就怎么来。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java线程]]></title>
    <url>%2F2020%2F03%2F18%2Fjava%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线程和进程一个程序就是一个进程，而一个程序中的多个任务则被称为线程。进程是表示资源分配的基本单位，线程是进程中执行运算的最小单位，亦是调度运行的基本单位。 线程基础如何创建一个线程在创建一个线程时，最好为这个线程设置线程名称，因为这样在使用jstack分析程序或者进行问题排查时，就会给开发人员提供一些提示1，直接继承Tread类，重写run()。2，实现Runnable结构，实现run()方法。3，实现Callable接口。通过FutureTask包装器来创建thread线程。4，ThreadPool。 线程的生命周期线程一共有 6 种状态(NEW、RUNNABLE、BLOCKED、 WAITING、TIME_WAITING、TERMINATED) NEW:初始状态，线程被构建，但是还没有调用 start 方法。 RUNNABLE:运行状态，JAVA 线程把操作系统中的就绪 和运行两种状态统一称为“运行中”。 BLOCKED:阻塞状态，表示线程进入等待状态,也就是线程 因为某种原因放弃了 CPU 使用权，阻塞也分为几种情况。➢等待阻塞:运行的线程执行 wait 方法，jvm 会把当前 线程放入到等待队列。 ➢同步阻塞:运行的线程在获取对象的同步锁时，若该同 步锁被其他线程锁占用了，那么 jvm 会把当前的线程 放入到锁池中。➢其他阻塞:运行的线程执行 Thread.sleep 或者 t.join 方 法，或者发出了 I/O 请求时，JVM 会把当前线程设置 为阻塞状态，当 sleep 结束、join 线程终止、io 处理完 毕则线程恢复。 WAITING 都是等待状态，与TIME_WAITING的区别只是不会超时。 TIME_WAITING:超时等待状态，超时以后自动返回。 TERMINATED:终止状态，表示当前线程执行完毕。 线程的启动调用start()方法去启动一个线程，当run方法中的代码执行完毕以后，线程的生命周期也将终止，调用start方法的语义是当前线程告诉JVM，启动调用start方法的线程。线程启动的原理，在Thread方法中，底层实际上个是调用了start0()的一个方法，，这个方法是在这个方法native修饰的一个底层方法 线程的终止线程的终止，并不是简单的调用 stop 命令去。虽然api仍然可以调用，但是和其他的线程控制方法如 suspend、 resume 一样都是过期了的不建议使用，就拿stop来说，stop方法在结束一个线程时并不会保证线程的资源正常释放，因此会导致程序可能出现一些不确定的状态。要优雅的去中断一个线程，在线程中提供了一个 interrupt 方法。 interrupt 方法 当其他线程通过调用当前线程的 interrupt 方法，表示向当前线程打个招呼，告诉他可以中断线程的执行了，至于什 么时候中断，取决于当前线程自己。 线程通过检查自身是否被中断来进行相应，可以通过 isInterrupted()来判断是否被中断。 Thread.interrupted()线程中还提供了静态方法 Thread.interrupted()对设置中断标识的线程复位。 其他的线程复位除了通过 Thread.interrupted 方法对线程中断标识进行复 位以外，还有一种被动复位的场景，就是对抛出 InterruptedException 异 常 的 方 法 ，在 InterruptedException 抛出之前，JVM 会先把线程的中断 标识位清除，然后才会抛出 InterruptedException，这个时 候如果调用 isInterrupted方法，将会返回false。 为什么要复位Thread.interrupted()是属于当前线程的，是当前线程对外 界中断信号的一个响应，表示自己已经得到了中断信号， 但不会立刻中断自己，具体什么时候中断由自己决定，让 外界知道在自身中断前，他的中断状态仍然是 false，这就 是复位的原因。需要注意的是，InterruptedException 异常的抛出并不意味 着线程必须终止，而是提醒当前线程有中断的操作发生， 至于接下来怎么处理取决于线程本身，比如 直接捕获异常不做任何处理 将异常往外抛出 停止当前线程，并打印异常信息 线程安全 一个对象是否是线程安全的，取决于它是否会被多个线程 访问，以及程序中是如何去使用这个对象的。所以，如果 多个线程访问同一个共享对象，在不需额外的同步以及调 用端代码不用做其他协调的情况下，这个共享对象的状态 依然是正确的(正确性意味着这个对象的结果与我们预期 规定的结果保持一致)，那说明这个对象是线程安全的。 理解锁的基础知识基础知识之一：锁的类型锁从宏观上分类，分为悲观锁与乐观锁。 乐观锁乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新），如果失败则要重复读-比较-写的操作。 java中的乐观锁基本都是通过CAS操作实现的，CAS是一种更新的原子操作，比较当前值跟传入值是否一样，一样则更新，否则失败。 悲观锁悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block直到拿到锁。java中的悲观锁就是Synchronized,AQS框架下的锁则是先尝试cas乐观锁去获取锁，获取不到，才会转换为悲观锁，如RetreenLock。 基础知识之二：java线程阻塞的代价java的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统介入，需要在户态与核心态之间切换，这种切换会消耗大量的系统资源，因为用户态与内核态都有各自专用的内存空间，专用的寄存器等，用户态切换至内核态需要传递给许多变量、参数给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。 如果线程状态切换是一个高频操作时，这将会消耗很多CPU处理时间；如果对于那些需要同步的简单的代码块，获取锁挂起操作消耗的时间比用户代码执行的时间还要长，这种同步策略显然非常糟糕的。synchronized会导致争用不到锁的线程进入阻塞状态，所以说它是java语言中一个重量级的同步操纵，被称为重量级锁，为了缓解上述性能问题，JVM从1.5开始，引入了轻量锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。 明确java线程切换的代价，是理解java中各种锁的优缺点的基础之一。 在介绍java锁之前，先说下什么是markword，markword是java对象数据结构中的一部分，要详细了解java对象的结构可以点击这里,这里只做markword的详细介绍，因为对象的markword和java各种类型的锁密切相关； markword数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，它的最后2bit是锁状态标志位，用来标记当前对象的状态，对象的所处的状态，决定了markword存储的内容，如下表所示: 32位虚拟机在不同状态下markword结构如下图所示： 基础知识之三：markwordsynchronized 的基本认识在多线程并发编程中 synchronized 一直是元老级角色，很 多人都会称呼它为重量级锁。但是，随着 Java SE 1.6 对 synchronized 进行了各种优化之后，有些情况下它就并不 那么重，Java SE 1.6 中为了减少获得锁和释放锁带来的性 能消耗而引入的偏向锁和轻量级锁。 synchronized 的基本语法synchronized 有三种方式来加锁，分别是 修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁 静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。不同的修饰类型，代表锁的控制粒度 锁是如何存储的 为什么任何对象都可以实现锁 首先，Java 中的每个对象都派生自 Object 类，而每个 Java Object 在 JVM 内部都有一个 native 的 C++对象 oop/oopDesc 进行对应。 线程在获取锁的时候，实际上就是获得一个监视器对象(monitor) ,monitor 可以认为是一个同步对象，所有的 Java 对象是天生携带 monitor。多个线程访问同步代码块时，相当于去争抢对象监视器 修改对象中的锁标识 synchronized锁的升级在分析 markword 时，提到了偏向锁、轻量级锁、重量级 锁。在分析这几种锁的区别时，我们先来思考一个问题 使用锁能够实现数据的安全性，但是会带来性能的下降。 不使用锁能够基于线程并行提升程序性能，但是却不能保 证线程安全性。这两者之间似乎是没有办法达到既能满足 性能也能满足安全性的要求。hotspot 虚拟机的作者经过调查发现，大部分情况下，加锁 的代码不仅仅不存在多线程竞争，而且总是由同一个线程 多次获得。所以基于这样一个概率，是的 synchronized 在 JDK1.6 之后做了一些优化，为了减少获得锁和释放锁带来 的性能开销，引入了偏向锁、轻量级锁的概念。因此大家 会发现在 synchronized 中，锁存在四种状态 分别是:无锁、偏向锁、轻量级锁、重量级锁; 锁的状态 根据竞争激烈的程度从低到高不断升级。 偏向锁偏向锁的基本原理 前面说过，大部分情况下，锁不仅仅不存在多线程竞争， 而是总是由同一个线程多次获得，为了让线程获取锁的代 价更低就引入了偏向锁的概念。怎么理解偏向锁呢? 当一个线程访问加了同步锁的代码块时，会在对象头中存 储当前线程的 ID，后续这个线程进入和退出这段加了同步 锁的代码块时，不需要再次加锁和释放锁。而是直接比较 对象头里面是否存储了指向当前线程的偏向锁。如果相等 表示偏向锁是偏向于当前线程的，就不需要再尝试获得锁了。 偏向锁的获取 首先获取锁 对象的 Markword，判断是否处于可偏向状 态。(biased_lock=1、且 ThreadId 为空) 如果是可偏向状态，则通过 CAS 操作，把当前线程的 ID 写入到 MarkWord a) 如果 cas 成功，那么 markword 就会变成这样。 表示已经获得了锁对象的偏向锁，接着执行同步代码块 b) 如果 cas 失败，说明有其他线程已经获得了偏向锁， 这种情况说明当前锁存在竞争，需要撤销已获得偏向锁的线程，并且把它持有的锁升级为轻量级锁(这个 操作需要等到全局安全点，也就是没有线程在执行字节码)才能执行 如果是已偏向状态，需要检查 markword 中存储的 ThreadID 是否等于当前线程 ThreadID a) 如果相等，不需要再次获得锁，可直接执行同步代码块. b) 如果不相等，说明当前锁偏向于其他线程，需要撤销 偏向锁并升级到轻量级锁. 偏向锁的撤销 偏向锁的撤销并不是把对象恢复到无锁可偏向状态(因为 偏向锁并不存在锁释放的概念)，而是在获取偏向锁的过程 中，发现 cas 失败也就是存在线程竞争时，直接把被偏向 的锁对象升级到被加了轻量级锁的状态。对原持有偏向锁的线程进行撤销时，原获得偏向锁的线程 有两种情况: 原获得偏向锁的线程如果已经退出了临界区，也就是同步代码块执行完了，那么这个时候会把对象头设置成无 锁状态并且争抢锁的线程可以基于 CAS 重新偏向当前线程。 如果原获得偏向锁的线程的同步代码块还没执行完，处于临界区之内，这个时候会把原获得偏向锁的线程升级为轻量级锁后继续执行同步代码块。 偏向锁流程图分析： 在我们的应用开发中，绝大部分情况下一定会存在2个以上的线程竞争，那么如果开启偏向锁，反而会提升获取锁的资源消耗。所以可以通过 jvm 参数 seBiasedLocking 来设置开启或关闭偏向锁 轻量锁锁升级为轻量级锁之后，对象的 Markword 也会进行相应 的的变化。升级为轻量级锁的过程: 线程在自己的栈桢中创建锁记录 LockRecord。 将锁对象的对象头中的 MarkWord 复制到线程的刚刚创建的锁记录中。 将锁记录中的 Owner 指针指向锁对象。 将锁对象的对象头的 MarkWord 替换为指向锁记录的指针。 自旋锁轻量级锁在加锁过程中，用到了自旋锁。所谓自旋，就是指当有另外一个线程来竞争锁时，这个线 程会在原地循环等待，而不是把该线程给阻塞，直到那个获得锁的线程释放锁之后，这个线程就可以马上获得锁的。 注意，锁在原地循环的时候，是会消耗 cpu 的，就相当于在执行一个啥也没有的 for 循环。所以，轻量级锁适用于那些同步代码块执行的很快的场景， 这样，线程原地等待很短的时间就能够获得锁了。 自旋锁的使用，其实也是有一定的概率背景，在大部分同 步代码块执行的时间都是很短的。所以通过看似无意义的循环反而能提升锁的性能。 但是自旋必须要有一定的条件控制，否则如果一个线程执行同步代码块的时间很长，那么这个线程不断的循环反而会消耗 CPU 资源。默认情况下自旋的次数是 10 次， 可以通过 preBlockSpin 来修改。 在 JDK1.6 之后，引入了自适应自旋锁，自适应意味着自旋 的次数不是固定不变的，而是根据前一次在同一个锁上自旋的时间以及锁的拥有者的状态来决定。 如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过， 那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源 轻量级锁的解锁轻量级锁的锁释放逻辑其实就是获得锁的逆向逻辑，通过 CAS 操作把线程栈帧中的 LockRecord 替换回到锁对象的 MarkWord 中，如果成功表示没有竞争。如果失败，表示当前锁存在竞争，那么轻量级锁就会膨胀成为重量级锁 流程图如下图： 重量级锁的基本原理当轻量级锁膨胀到重量级锁之后，意味着线程只能被挂起 阻塞来等待被唤醒了。 重量级锁的 monitor加了同步代码块以后，在字节码中会看到一个 monitorenter 和 monitorexit。每一个 JAVA 对象都会与一个监视器 monitor 关联，我们 可以把它理解成为一把锁，当一个线程想要执行一段被 synchronized 修饰的同步方法或者代码块时，该线程得先 获取到 synchronized 修饰的对象对应的 monitor。monitorenter 表示去获得一个对象监视器。monitorexit 表 示释放 monitor 监视器的所有权，使得其他被阻塞的线程 可以尝试去获得这个监视器。monitor 依赖操作系统的 MutexLock(互斥锁)来实现的, 线程被阻塞后便进入内核(Linux)调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能。 重量级锁的加锁的基本流程任意线程对 Object(Object 由 synchronized 保护)的访问，首先要获得 Object 的监视器。如果获取失败，线程进 入同步队列，线程状态变为 BLOCKED。当访问 Object 的前驱(获得了锁的线程)释放了锁，则该释放操作唤醒阻 塞在同步队列中的线程，使其重新尝试对监视器的获取。 重量级锁的流程： 整体梳理线程的竞争机制再来回顾一下线程的竞争机制对于锁升级这块的一些基本 流程。方便更好的理解 加入有这样一个同步代码块，存在 Thread#1、Thread#2 等多个线程 123synchronized (lock) &#123; // do something&#125; 情况一:只有 Thread#1 会进入临界区; 情况二:Thread#1 和 Thread#2 交替进入临界区,竞争不激烈; 情况三:Thread#1/Thread#2/Thread3… 同时进入临界区，竞争激烈 偏向锁 此时当 Thread#1 进入临界区时，JVM 会将 lockObject 的 对象头 Mark Word 的锁标志位设为“01”，同时会用 CAS 操作把 Thread#1 的线程 ID 记录到 Mark Word 中，此时进 入偏向模式。所谓“偏向”，指的是这个锁会偏向于 Thread#1， 若接下来没有其他线程进入临界区，则 Thread#1 再出入 临界区无需再执行任何同步操作。也就是说，若只有 Thread#1 会进入临界区，实际上只有 Thread#1 初次进入 临界区时需要执行 CAS 操作，以后再出入临界区都不会有 同步操作带来的开销。 轻量级锁 偏向锁的场景太过于理想化，更多的时候是 Thread#2 也 会尝试进入临界区， 如果 Thread#2 也进入临界区但是 Thread#1 还没有执行完同步代码块时，会暂停 Thread#1 并且升级到轻量级锁。Thread#2 通过自旋再次尝试以轻量 级锁的方式来获取锁 重量级锁 如果 Thread#1 和 Thread#2 正常交替执行，那么轻量级锁 基本能够满足锁的需求。但是如果 Thread#1 和 Thread#2 同时进入临界区，那么轻量级锁就会膨胀为重量级锁，意 味着 Thread#1 线程获得了重量级锁的情况下，Thread#2 就会被阻塞 Synchronized 结合 Java Object 对象中的 wait,notify,notifyAll被阻塞的线程什么时候被唤醒，取决于获得锁的线程什么时候执行完同步代码块并且释放锁。那怎么做到显示控制呢?我们就需要 借助一个信号机制: 在 Object 对象中，提供了 wait/notify/notifyall，可以用于控制线程的状态。 wait/notify/notifyall 基本概念 wait:表示持有对象锁的线程 A 准备释放对象锁权限，释放 cpu 资源并进入等待状态。 notify:表示持有对象锁的线程 A 准备释放对象锁权限，通 知 jvm 唤醒某个竞争该对象锁的线程 X。线程 A synchronized 代码执行结束并且释放了锁之后，线程 X 直 接获得对象锁权限，其他竞争线程继续等待(即使线程 X 同 步完毕，释放对象锁，其他竞争线程仍然等待，直至有新 的 notify ,notifyAll 被调用)。 notifyAll:notifyall 和 notify 的区别在于，notifyAll 会唤醒 所有竞争同一个对象锁的所有线程，当已经获得锁的线程 A 释放锁之后，所有被唤醒的线程都有可能获得对象锁权 限 需要注意的是:三个方法都必须在 synchronized 同步关键 字所限定的作用域中调用，否则会报错 java.lang.IllegalMonitorStateException ，意思是因为没有同步，所以线程对对象锁的状态是不确定的，不能调用这 些方法。 另外，通过同步机制来确保线程从 wait 方法返回时能够感知到感知到 notify 线程对变量做出的修改 wait/notify 的基本使用12345678910111213141516171819202122232425262728293031323334353637383940414243public class ThreadA extends Thread&#123; private Object lock; public ThreadA(Object lock) &#123; this.lock = lock; &#125; @Override public void run() &#123; synchronized (lock)&#123; System.out.println("start ThreadA"); try &#123; lock.wait(); //实现线程的阻塞 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("end ThreadA"); &#125; &#125;&#125;public class ThreadB extends Thread&#123; private Object lock=new Object(); public ThreadB(Object lock) &#123; this.lock = lock; &#125; @Override public void run() &#123; synchronized (lock)&#123; System.out.println("start ThreadB"); lock.notify(); //唤醒被阻塞的线程 System.out.println("end ThreadB"); &#125; &#125;&#125;public class WaitNotifyDemo &#123; public static void main(String[] args) &#123; Object lock=new Object(); ThreadA threadA=new ThreadA(lock); threadA.start(); ThreadB threadB=new ThreadB(lock); threadB.start(); &#125;&#125; wait/notify 的基本原理 Volatilevolatile 可以使得在多处理器环境下保证了共享变量的可见性。 在单线程的环境下，如果向一个变量先写入一个值，然后在没有写干涉的情况下读取这个变量的值，那这个时候读取到的这个变量的值应该是之前写入的那个值。这本来是一个很正常的事情。但是在多线程环境下，读和写发生在不同的线程中的时候，可能会出现:读线程不能及时的读取到其他线程写入的最新的值。这就是所谓的可见性为了实现跨线程写入的内存可见性，必须使用到一些机制 来实现。而 volatile 就是这样一种机制。 123456789101112131415public class VolatileDemo &#123; public /*volatile*/ static boolean stop = false; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(()-&gt;&#123; int i = 0; while (!stop)&#123; i++; &#125; &#125;); thread.start(); System.out.println("begin start thread"); Thread.sleep(1000); stop = true; &#125;&#125; Thread.join123456789101112131415161718192021222324public class JoinTest &#123; public static void main(String [] args) throws InterruptedException &#123; ThreadJoinTest t1 = new ThreadJoinTest("小明"); ThreadJoinTest t2 = new ThreadJoinTest("小东"); t1.start(); /**join的意思是使得放弃当前线程的执行，并返回对应的线程，例如下面代码的意思就是： 程序在main线程中调用t1线程的join方法，则main线程放弃cpu控制权，并返回t1线程继续执行直到线程t1执行完毕 所以结果是t1线程执行完后，才到主线程执行，相当于在main线程中同步t1线程，t1执行完了，main线程才有执行的机会 */ t1.join(); t2.start(); &#125;&#125;class ThreadJoinTest extends Thread&#123; public ThreadJoinTest(String name)&#123; super(name); &#125; @Override public void run()&#123; for(int i=0;i&lt;1000;i++)&#123; System.out.println(this.getName() + ":" + i); &#125; &#125;&#125; JMMLockJava.util.concurrent 是在并发编程中比较常用的工具类，里面包含很多用来在并发 场景中使用的组件。比如线程池、阻塞队列、计时器、同步器、并发集合等等。并 发包的作者是大名鼎鼎的 Doug Lea。 Lock的基本使用以及原理分析Lock 简介在 Lock 接口出现之前，Java 中的应用程序对于多线程的并发安全处理只能基于 synchronized 关键字来解决。但是 synchronized 在有些场景中会存在一些短板，就是它并不适合于所有的并发场景。但是在 Java5 以后，Lock 的出现可以解决 synchronized 在某些场景中的短板，它比 synchronized 更加灵活。 Lock 的实现Lock 本质上是一个接口，它定义了释放锁和获得锁的抽象方法，定义成接口就意 味着它定义了锁的一个标准规范，也同时意味着锁的不同实现。实现 Lock 接口的类有很多，以下为几个常见的锁实现: ReentrantLock:表示重入锁，它是唯一一个实现了 Lock 接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数 ReentrantReadWriteLock:重入读写锁，它实现了 ReadWriteLock 接口，在这个类中维护了两个锁，一个是 ReadLock，一个是 WriteLock，他们都分别实现了 Lock 接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则 是: 读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的 操作都会存在互斥。 StampedLock: stampedLock 是 JDK8 引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。 stampedLock 是一种乐观的读策略，使得乐观锁完全不会阻塞写线程。 Lock的类关系图Lock 有很多的锁的实现，但是直观的实现是 ReentrantLock 重入锁 ReentrantLock 重入锁重入锁，表示支持重新进入的锁，也就是说，如果当前线程 t1 通过调用 lock 方 法获取了锁之后，再次调用 lock，是不会再阻塞去获取锁的，直接增加重试次数 就行了。synchronized 和 ReentrantLock 都是可重入锁。 重入锁的设计目的比如调用 demo 方法获得了当前的对象锁，然后在这个方法中再去调用 demo2，demo2 中的存在同一个实例锁，这个时候当前线程会因为无法获得 demo2 的对象锁而阻塞，就会产生死锁。重入锁的设计目的是避免线程的死锁。 123456789101112131415public class ReentrantDemo&#123; public synchronized void demo()&#123; System.out.println("begin:demo"); demo2(); &#125; public void demo2()&#123; System.out.println("begin:demo1"); synchronized (this)&#123; &#125; &#125; public static void main(String[] args) &#123; ReentrantDemo rd=new ReentrantDemo(); new Thread(rd::demo).start(); &#125;&#125; ReentrantLock 的实现原理 我们知道锁的基本原理是，基于将多线程并行任务通过某一种机制实现线程的串行执行，从而达到线程安全性的目的。在 synchronized 中，我们分析了偏向锁、 轻量级锁、乐观锁。基于乐观锁以及自旋锁来优化了 synchronized 的加锁开销， 同时在重量级锁阶段，通过线程的阻塞以及唤醒来达到线程竞争和同步的目的。 那么在 ReentrantLock 中，也一定会存在这样的需要去解决的问题。就是在多线程竞争重入锁时，竞争失败的线程是如何实现阻塞以及被唤醒的呢? AQS 是什么 在 Lock 中，用到了一个同步队列 AQS，全称 AbstractQueuedSynchronizer，它 是一个同步工具也是 Lock 用来实现线程同步的核心组件。如果你搞懂了 AQS，那 么 J.U.C 中绝大部分的工具都能轻松掌握。 AQS 的两种功能从使用层面来说，AQS 的功能分为两种:独占和共享 独占锁，每次只能有一个线程持有锁，比如前面给大家演示的 ReentrantLock 就是以独占方式实现的互斥锁 共享锁，允许多个线程同时获取锁，并发访问共享资源，比如 ReentrantReadWriteLock AQS 的内部实现 AQS 队列内部维护的是一个 FIFO 的双向链表，这种结构的特点是每个数据结构 都有两个指针，分别指向直接的后继节点和直接前驱节点。所以双向链表可以从任意一个节点开始很方便的访问前驱和后继。每个 Node 其实是由线程封装，当线 程争抢锁失败后会封装成 Node 加入到 ASQ 队列中去;当获取锁的线程释放锁以 后，会从队列中唤醒一个阻塞的节点(线程)。 ReentrantLock 的源码分析 以 ReentrantLock 作为切入点，来看看在这个场景中是如何使用 AQS 来实现线程 的同步的 ReentrantLock 的时序图 调用 ReentrantLock 中的 lock()方法，源码的调用过程我使用了时序图来展现。 ReentrantLock.lock()这个是 reentrantLock 获取锁的入口 123public void lock() &#123; sync.lock();&#125; sync 实际上是一个抽象的静态内部类，它继承了 AQS 来实现重入锁的逻辑，我们前面说过 AQS 是一个同步队列，它能够实现线程的阻塞以及唤醒，但它并不具备业务功能，所以在不同的同步场景中，会继承 AQS 来实现对应场景的功能Sync 有两个具体的实现类，分别是: NofairSync:表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁FailSync: 表示所有线程严格按照 FIFO 来获取锁 NofairSync.lock以非公平锁为例，来看看 lock 中的实现 非公平锁和公平锁最大的区别在于，在非公平锁中我抢占锁的逻辑是，不管有没有线程排队，我先上来 cas 去抢占一下 CAS 成功，就表示成功获得了锁 CAS 失败，调用 acquire(1)走锁竞争逻辑 ReentrantReadWriteLock我们以前理解的锁，基本都是排他锁，也就是这些锁在同一时刻只允许一个线程进 行访问，而读写所在同一时刻可以允许多个线程访问，但是在写线程访问时，所有 的读线程和其他写线程都会被阻塞。读写锁维护了一对锁，一个读锁、一个写锁; 一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读 多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量。 Lock阻塞以及唤醒Condition Condition 是一个多线程协调通信的工具类，可以让某些线 程一起等待某个条件(condition)，只有满足条件时，线程 才会被唤醒 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ConditionWait implements Runnable&#123; private Lock lock; private Condition condition; public ConditionWait(Lock lock, Condition condition) &#123; this.lock = lock; this.condition = condition; &#125; @Override public void run() &#123; try &#123; lock.lock(); //竞争锁 try &#123; System.out.println("begin - ConditionWait"); condition.await();//阻塞(1. 释放锁, 2.阻塞当前线程, FIFO（单向、双向）) System.out.println("end - ConditionWait"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;finally &#123; lock.unlock();//释放锁 &#125; &#125;&#125;public class ConditionNotify implements Runnable&#123; private Lock lock; private Condition condition; public ConditionNotify(Lock lock, Condition condition) &#123; this.lock = lock; this.condition = condition; &#125; @Override public void run() &#123; try&#123; lock.lock();//获得了锁. System.out.println("begin - conditionNotify"); condition.signal();//唤醒阻塞状态的线程 // condition.await(); System.out.println("end - conditionNotify"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); //释放锁 &#125; &#125;&#125;public class App&#123; public static void main( String[] args )&#123; Lock lock=new ReentrantLock(); //重入锁 Condition condition=lock.newCondition(); new Thread(new ConditionWait(lock,condition)).start();// 阻塞await new Thread(new ConditionNotify(lock,condition)).start(); &#125;&#125; 线程池在 Java 中，如果每个请求到达就创建一个新线程，创建和销毁线程花费的时间和消耗的系统 资源都相当大，甚至可能要比在处理实际的用户请求的时间和资源要多的多。如果在一个 Jvm 里创建太多的线程，可能会使系统由于过度消耗内存或“切换过度”而导致系 统资源不足 为了解决这个问题,就有了线程池的概念. 什么是线程池线程池的核心逻辑是提前创建好若干个线程放在一 个容器中。如果有任务需要处理，则将任务直接分配给线程池中的线程来执行就行，任务处 理完以后这个线程不会被销毁，而是等待后续分配任务。同时通过线程池来重复管理线程还 可以避免创建大量线程增加开销。 合理的使用线程池，可以带来一些好处 降低创建线程和销毁线程的性能开销 提高响应速度，当有新任务需要执行是不需要等待线程创建就可以立马执行 3. 合理的设置线程池大小可以避免因为线程数超过硬件资源瓶颈带来的问题 java中提供的线程池Executors 里面提供了几个线程池的工厂方法， newFixedThreadPool:该方法返回一个固定数量的线程池，线程数不变，当有一个任务提交 时，若线程池中空闲，则立即执行，若没有，则会被暂缓在一个任务队列中，等待有空闲的 线程去执行。 newSingleThreadExecutor: 创建一个线程的线程池，若空闲则执行，若没有空闲线程则暂缓 在任务队列中。 newCachedThreadPool:返回一个可根据实际情况调整线程个数的线程池，不限制最大线程 数量，若用空闲的线程则执行任务，若无任务则不创建线程。并且每一个空闲线程会在 60 秒 后自动回收 newScheduledThreadPool: 创建一个可以指定线程的数量的线程池，但是这个线程池还带有 延迟和周期性执行任务的功能，类似定时器。 ThreadpoolExecutor上面提到的四种线程池的构建，都是基于 ThreadpoolExecutor 来构建的 12345678public ThreadPoolExecutor(int corePoolSize, //核心线程数量 int maximumPoolSize, //最大线程数 long keepAliveTime, //超时时间,超出核心线程数量以外的线程空余存活时间 TimeUnit unit, //存活时间单位 BlockingQueue&lt;Runnable&gt; workQueue, //保存执行任务的队列 ThreadFactory threadFactory,//创建新线程使用的工厂 RejectedExecutionHandler handler //当任务无法执行的时候的处理方式) 线程池初始化时是没有创建线程的，线程池里的线程的初始化与其他线程一样，但是在完成 任务以后，该线程不会自行销毁，而是以挂起的状态返回到线程池。直到应用程序再次向线 程池发出请求时，线程池里挂起的线程就会再度激活执行任务。这样既节省了建立线程所造 成的性能损耗，也可以让多个任务反复重用同一线程，从而在应用程序生存期内节约大量开 销 newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; FixedThreadPool 的核心线程数和最大线程数都是指定值，也就是说当线程池中的线程数超 过核心线程数后，任务都会被放到阻塞队列中。另外 keepAliveTime 为 0，也就是超出核心 线程数量以外的线程空余存活时间而这里选用的阻塞队列是 LinkedBlockingQueue，使用的是默认容量 Integer.MAX_VALUE， 相当于没有上限 这个线程池执行任务的流程如下: 线程数少于核心线程数，也就是设置的线程数时，新建线程执行任务 线程数等于核心线程数后，将任务加入阻塞队列 由于队列容量非常大，可以一直添加 执行完任务的线程反复去队列中取任务执行 用途:FixedThreadPool 用于负载比较大的服务器，为了资源的合理利用，需要限制当前线 程数量 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定 顺序(FIFO, LIFO, 优先级)执行 newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; CachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空 闲线程，若无可回收，则新建线程; 并且没有核心线程，非核心线程数无上限，但是每个空闲 的时间只有 60 秒，超过后就会被回收。它的执行流程如下: 没有核心线程，直接向 SynchronousQueue 中提交任务 如果有空闲线程，就去取出任务执行;如果没有空闲线程，就新建一个 执行完任务的线程有 60 秒生存时间，如果在这个时间内可以接到新任务，就可以继续活下去，否则就被回收 newScheduledThreadPoolScheduledThreadPoolExecutor 继承了 ThreadPoolExecutor，并另外提供一些调度方法以支 持定时和周期任务。 线程池的实现原理线程问题如何排查常见的面试题如何保证多个线程串行执行thread.join 线程池的实现原理sleep和wait的区别Synchronized 和Lock的区别如何有效避免死锁死锁产生的原因：两个或多个线程各自持有对方线程等待释放的锁。如何避免：提供一个有序性的资源锁定和 ConcurrentHashMap的实现原理线程间如何通讯volatile 关键字在什么场景下使用。在高并发场景下，保证共享变量的可见性。]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java集合]]></title>
    <url>%2F2020%2F03%2F15%2Fjava%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[java中集合大纲首先我们看结构类图 在java中，主要是collection和map两个集合接口，我们在java中使用到的所有集合都是继承或者实现了这两个接口。 CollectionCollection 接口中定义的方法12345678910111213141516171819202122232425262728293031323334353637int size(); //返回集合中元素个数boolean isEmpty(); //判断集合是否为空boolean contains(Object o); //判断集合中是否包含某元素Iterator&lt;E&gt; iterator(); //迭代器Object[] toArray(); //返回一个包含了本集合类中所有元素的数组。&lt;T&gt; T[] toArray(T[] a);boolean add(E e); //在集合末尾添加元素boolean remove(Object o); //若当前集合中有值与0的值相等的元素，则删除boolean containsAll(Collection&lt;?&gt; c);boolean addAll(Collection&lt;? extends E&gt; c);boolean removeAll(Collection&lt;?&gt; c);default boolean removeIf(Predicate&lt;? super E&gt; filter) &#123; Objects.requireNonNull(filter); boolean removed = false; final Iterator&lt;E&gt; each = iterator(); while (each.hasNext()) &#123; if (filter.test(each.next())) &#123; each.remove(); removed = true; &#125; &#125; return removed; &#125; boolean retainAll(Collection&lt;?&gt; c); void clear(); boolean equals(Object o); int hashCode(); @Override default Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, 0); &#125; default Stream&lt;E&gt; stream() &#123; return StreamSupport.stream(spliterator(), false); &#125; default Stream&lt;E&gt; parallelStream() &#123; return StreamSupport.stream(spliterator(), true); &#125; 常用Collection的实现类Collection 接口的接口 对象的集合（单列集合）├——-—-List 接口：元素按进入先后有序保存，可重复│—————- ├ LinkedList 接口实现类， 链表结构， 插入删除， 没有同步， 线程不安全│—————- ├ ArrayList 接口实现类， 数组结构， 随机访问， 没有同步， 线程不安全│—————- └ Vector 接口实现类 数组， 同步， 线程安全│ ———————-└ Stack 是Vector类的实现类└——-Set 接口： 仅接收一次，不可重复，并做内部排序├————└HashSet 使用hash表（数组）存储元素│————————└ LinkedHashSet 链表维护元素的插入次序└ —————-TreeSet 底层实现为二叉树，元素排好序 ListSetHashSetHashSet底层数据结构采用哈希表实现，元素无序且唯一，线程不安全，效率高，可以存储null元素，元素的唯一性是靠所存储元素类型是否重写hashCode()和equals()方法来保证的，如果没有重写这两个方法，则无法保证元素的唯一性。具体实现唯一性的比较过程：存储元素首先会使用hash()算法函数生成一个int类型hashCode散列值，然后已经的所存储的元素的hashCode值比较，如果hashCode不相等，则所存储的两个对象一定不相等，此时存储当前的新的hashCode值处的元素对象；如果hashCode相等，存储元素的对象还是不一定相等，此时会调用equals()方法判断两个对象的内容是否相等，如果内容相等，那么就是同一个对象，无需存储；如果比较的内容不相等，那么就是不同的对象，就该存储了，此时就要采用哈希的解决地址冲突算法，在当前hashCode值处类似一个新的链表， 在同一个hashCode值的后面存储存储不同的对象，这样就保证了元素的唯一性。 LinkedHashSetLinkedHashSet底层数据结构采用链表和哈希表共同实现，链表保证了元素的顺序与存储顺序一致，哈希表保证了元素的唯一性。线程不安全，效率高。 TreeSetTreeSet底层数据结构采用二叉树来实现，元素唯一且已经排好序；唯一性同样需要重写hashCode和equals()方法，二叉树结构保证了元素的有序性。根据构造方法不同，分为自然排序（无参构造）和比较器排序（有参构造），自然排序要求元素必须实现Compareable接口，并重写里面的compareTo()方法，元素通过比较返回的int值来判断排序序列，返回0说明两个对象相同，不需要存储；比较器排需要在TreeSet初始化是时候传入一个实现Comparator接口的比较器对象，或者采用匿名内部类的方式new一个Comparator对象，重写里面的compare()方法； List和Set总结：（1）、List,Set都是继承自Collection接口.（2）、List特点：元素有放入顺序，元素可重复 ，Set特点：元素无放入顺序，元素不可重复，重复元素会覆盖掉，（注意：元素虽然无放入顺序，但是元素在set中的位置是有该元素的HashCode决定的，其位置其实是固定的，加入Set 的Object必须定义equals()方法 ，另外list支持for循环，也就是通过下标来遍历，也可以用迭代器，但是set只能用迭代，因为他无序，无法用下标来取得想要的值。）（3）.Set和List对比：Set：检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。List：和数组类似，List可以动态增长，查找元素效率高，插入删除元素效率低，因为会引起其他元素位置改变。（4）、ArrayList与LinkedList的区别和适用场景Arraylist：优点：ArrayList是实现了基于动态数组的数据结构,因为地址连续，一旦数据存储好了，查询操作效率会比较高（在内存里是连着放的）。缺点：因为地址连续， ArrayList要移动数据,所以插入和删除操作效率比较低。LinkedList：优点：LinkedList基于链表的数据结构,地址是任意的，所以在开辟内存空间的时候不需要等一个连续的地址，对于新增和删除操作add和remove，LinedList比较占优势。LinkedList 适用于要头尾操作或插入指定位置的场景缺点：因为LinkedList要移动指针,所以查询操作性能比较低。适用场景分析：当需要对数据进行对此访问的情况下选用ArrayList，当需要对数据进行多次增加删除修改时采用LinkedList。 Vector Vector 线程安全 与arraylist的实现一样也是一个动态数组。但是因为他很多方法用了synchronized来修饰是线程同步的，效率很低，一般不建议使用 常见的面试题如何解决arraylist线程不安全的问题1，使用Vector2，使用Collections.synchronizedList。它会自动将我们的list方法进行改变，最后返回给我们一个加锁了List 1protected static List&lt;Object&gt; arrayListSafe2 = Collections.synchronizedList(new ArrayList&lt;Object&gt;()); 3,使用JUC中的CopyOnWriteArrayList类进行替换。 Map Map用于保存具有映射关系的数据，Map里保存着两组数据：key和value，它们都可以使任何引用类型的数据，但key不能重复。所以通过指定的key就可以取出对应的value。 Map中定义的方法12345678910111213int size();boolean isEmpty();boolean containsKey(Object key);// 查询Map中是否包含指定的key，如果包含则返回trueboolean containsValue(Object value); // 查询Map中是否包含指定value，如果包含则返回trueV get(Object key); //获取对象V put(K key, V value); //设置对象V remove(Object key); //移除对象void putAll(Map&lt;? extends K, ? extends V&gt; m);void clear(); //清空集合Set&lt;K&gt; keySet(); //返回所有key组成的set集合Collection&lt;V&gt; values(); // 返回Map中所有的values Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet(); 返回set集合，里面是当前map集合中的entry... Map 常用的实现类Map 接口 键值对的集合 （双列集合）├———Hashtable 接口实现类， 同步， 线程安全├———HashMap 接口实现类 ，没有同步， 线程不安全-│—————–├ LinkedHashMap 双向链表和哈希表实现│—————–└ WeakHashMap├ ——–TreeMap 红黑树对所有的key进行排序└———IdentifyHashMap———————————————— HashMapjdk1.7 底层是数组+链表jdk1.8 底层是数组+链表+红黑树实现非线程安全 //todo 画出hashmap.put的流程图 HashTableStringBuilder是非线程安全的，StringBuffer是线程安全的。 部分内容参考自 ：https://blog.csdn.net/feiyanaffection/article/details/81394745]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM基础2]]></title>
    <url>%2F2019%2F12%2F20%2FJVM%E5%9F%BA%E7%A1%802%2F</url>
    <content type="text"><![CDATA[1 Java虚拟机栈和栈帧 官网 :https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-2.html#jvms-2.6 栈帧:每个栈帧对应一个被调用的方法，可以理解为一个方法的运行空间。 每个栈帧中包括局部变量表(Local Variables)、操作数栈(Operand Stack)、指向运行时常量池的引用(A reference to the run-time constant pool)、方法返回地址(Return Address)和附加信息。 局部变量表:方法中定义的局部变量以及方法的参数存放在这张表中 局部变量表中的变量不可直接使用，如需要使用的话，必须通过相关指令将其加载至操作数栈中作为操作数使用。 操作数栈:以压栈和出栈的方式存储操作数的 动态链接:每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态 连接(Dynamic Linking)。 方法返回地址:当一个方法开始执行后,只有两种方式可以退出，一种是遇到方法返回的字节码指令;一种是遇见异常，并且 这个异常没有在方法体内得到处理。 2 堆栈之间的关系2.1栈指向堆如果在栈帧中有一个变量，类型为引用类型，比如Object obj=new Object()，这时候就是典型的栈中元素指向堆中的 对象。 2.2 方法区指向堆方法区中会存放静态变量，常量等数据。如果是下面这种情况，就是典型的方法区中元素指向堆中的对象。 1private static Object obj=new Object(); 2.3 堆指向方法区方法区中会包含类的信息，堆中会有对象，那怎么知道对象是哪个类创建的呢?]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM基础1]]></title>
    <url>%2F2019%2F12%2F19%2FJVM%E5%9F%BA%E7%A1%801%2F</url>
    <content type="text"><![CDATA[1 java jdk 1.81.1 The relation of JDK/JRE/JVMjdk、jre、jvm的关系可以用下图表示 2源码到类文件2.1源码123456789101112class Person&#123; private String name; private int age; private static String address; private final static String hobby=&quot;Programming&quot;; public void say()&#123; System.out.println(&quot;person say...&quot;); &#125; public int calc(int op1,int op2)&#123; return op1+op2; &#125;&#125; 编译: javac Person.java —&gt; Person.class 2.2编译过程Person.java -&gt; 词法分析器 -&gt; tokens流 -&gt; 语法分析器 -&gt; 语法树/抽象语法树 -&gt; 语义分析器 -&gt; 注解抽象语法树 -&gt; 字节码生成器 -&gt; Person.class文件 2.3类文件(Class文件)官网The class File Format :https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-4.html 123cafe babe 0000 0034 0027 0a00 0600 1809 0019 001a 0800 1b0a 001c 001d 0700 1e07 001f 0100 046e 616d 6501 0012 4c6a 6176 612f 6c61 6e67 2f53 7472 696e 673b 0100 0361 6765 0100 0149 0100 0761 6464 7265 ...... magic(魔数):The magic item supplies the magic number identifying the class file format; it has thevalue 0xCAFEBABE . 3类文件到虚拟机(类加载机制)3.1装载(Load)查找和导入class文件 1.通过一个类的全限定名获取定义此类的二进制字节流 2.将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 3.在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口 3.2链接(Link)3.2.1验证(Verify) 保证被加载类的正确性 文件格式验证 元数据验证 字节码验证 符号引用验证 3.2.2准备（Prepare） 为类的静态变量分配内存，并将其初始化为默认值 3.2.3解析（Resolve） 把类中的符号引用转换为直接引用 3.3初始化（Initialize） 对类的静态变量，静态代码块执行初始化操作 3.4类加载机制图解 使用和卸载不算是类加载过程中的阶段，这里只是为了完整性 4类装载器ClassLoader 在装载(Load)阶段，其中第(1)步:通过类的全限定名获取其定义的二进制字节流，需要借助类装载器完成，顾名思义，就是用来装载Class文件的。(1)通过一个类的全限定名获取定义此类的二进制字节流 4.1分类12341)Bootstrap ClassLoader 负责加载$JAVA_HOME中 jre/lib/rt.jar 里所有的class或 Xbootclassoath选项指定的jar包。由C++实现，不是ClassLoader子类。2)Extension ClassLoader 负责加载java平台中扩展功能的一些jar包，包括$JAVA_HOME中 jre/lib/*.jar 或 -Djava.ext.dirs指定目录下的jar包。3)App ClassLoader 负责加载classpath中指定的jar包及 Djava.class.path 所指定目录下的类和 jar包。4)Custom ClassLoader 通过java.lang.ClassLoader的子类自定义加载class，属于应用程序根据 自身需要自定义的ClassLoader，如tomcat、jboss都会根据j2ee规范自行实现ClassLoader。 4.2图解 4.3加载原则检查某个类是否已经加载:顺序是自底向上，从Custom ClassLoader到BootStrap ClassLoader逐层检查，只要某个Classloader已加载，就视为已加载此类，保证此类只所有ClassLoader加载一次。 加载的顺序:加载的顺序是自顶向下，也就是由上层来逐层尝试加载此类。 双亲委派机制 定义:如果一个类加载器在接到加载类的请求时，它首先不会自己尝试去加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归，如果父类加载器可以完成类加载任务，就 成功返回;只有父类加载器无法完成此加载任务时，才自己去加载。优势:Java类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java中的 Object类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，最终都是委派给处于模型 最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类。如果不采用 双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object 类。破坏:可以继承ClassLoader类，然后重写其中的loadClass方法，其他方式大家可以自己了解 拓展一下。 5运行时数据区(Run-Time Data Areas) 在装载阶段的第(2),(3)步可以发现有运行时数据，堆，方法区等名词(2)将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 (3)在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口 说白了就是类文件被类装载器装载进来之后，类中的内容(比如变量，常量，方法，对象等这些数 据得要有个去处，也就是要存储起来，存储的位置肯定是在JVM中有对应的空间) 5.1官网概括https://docs.oracle.com/javase/specs/jvms/se8/html/index.html 摘要 1The Java Virtual Machine defines various run-time data areas that are used during execution of a program. Some of these data areas are created on Java Virtual Machine start-up and are destroyed only when the Java Virtual Machine exits. Other data areas are per thread. Per-thread data areas are created when a thread is created and destroyed when the thread exits. 5.2图解 5.3常规理解5.3.1Method Area(方法区)方法区是各个线程共享的内存区域，在虚拟机启动时创建。 用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却又一个别名叫做Non-Heap(非堆)，目 的是与Java堆区分开来。当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 1234The Java Virtual Machine has a method area that is shared among all Java Virtual Machine threads.The method area is created on virtual machine start-up.Although the method area is logically part of the heap,......If memory in the method area cannot be made available to satisfy an allocation request, the Java Virtual Machine throws an OutOfMemoryError. 此时回看装载阶段的第2步:(2)将这个字节流所代表的静态存储结构转化为方法区的运行时数据 结构 如果这时候把从Class文件到装载的第(1)和(2)步合并起来理解的话，可以画个图 值得说明的 (1)方法区在JDK 8中就是Metaspace，在JDK6或7中就是Perm Space (2)Run-Time Constant PoolClass文件中除了有类的版本、字段、方法、接口等描述 信息外，还有一项信息就是常量池，用于存放编译时期生成的各种字面量和符号引用，这部分内容将在 类加载后进入方法区的运行时常量池中存放。 1Each run-time constant pool is allocated from the Java Virtual Machine&apos;s method area (§2.5.4).s 5.3.2Heap(堆)Java堆是Java虚拟机所管理内存中最大的一块，在虚拟机启动时创建，被所有线程共享。Java对象实例以及数组都在堆上分配。 12The Java Virtual Machine has a heap that is shared among all Java Virtual Machine threads. The heap is the run-time data area from which memory for all class instances and arrays is allocated.The heap is created on virtual machine start-up. 此时回看装载阶段的第3步:(3)在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方 法区中这些数据的访问入口 此时装载(1)(2)(3)的图可以改动一下 5.3.3Java Virtual Machine Stacks(虚拟机栈) 经过上面的分析，类加载机制的装载过程已经完成，后续的链接，初始化也会相应的生效。假如目前的阶段是初始化完成了，后续做啥呢?肯定是Use使用咯，不用的话这样折腾来折腾去 有什么意义?那怎样才能被使用到?换句话说里面内容怎样才能被执行?比如通过主函数main调 用其他方法，这种方式实际上是main线程执行之后调用的方法，即要想使用里面的各种内容，得 要以线程为单位，执行相应的方法才行。那一个线程执行的状态如何维护?一个线程可以执行多少个方法?这样的关系怎么维护呢? 虚拟机栈是一个线程执行的区域，保存着一个线程中方法的调用状态。换句话说，一个Java线程的运行 状态，由一个虚拟机栈来保存，所以虚拟机栈肯定是线程私有的，独有的，随着线程的创建而创建。每一个被线程执行的方法，为该栈中的栈帧，即每个方法对应一个栈帧。 调用一个方法，就会向栈中压入一个栈帧;一个方法调用完成，就会把该栈帧从栈中弹出。 1Each Java Virtual Machine thread has a private Java Virtual Machine stack, created at the same time as the thread. A Java Virtual Machine stack stores frames (§2.6). 画图理解栈和栈帧 5.3.4The pc Register(程序计数器) 我们都知道一个JVM进程中有多个线程在执行，而线程中的内容是否能够拥有执行权，是根据 CPU调度来的。假如线程A正在执行到某个地方，突然失去了CPU的执行权，切换到线程B了，然后当线程A再获 得CPU执行权的时候，怎么能继续执行呢?这就是需要在线程中维护一个变量，记录线程执行到 的位置。 程序计数器占用的内存空间很小，由于Java虚拟机的多线程是通过线程轮流切换，并分配处理器执行时 间的方式来实现的，在任意时刻，一个处理器只会执行一条线程中的指令。因此，为了线程切换后能够 恢复到正确的执行位置，每条线程需要有一个独立的程序计数器(线程私有)。如果线程正在执行Java方法，则计数器记录的是正在执行的虚拟机字节码指令的地址; 如果正在执行的是Native方法，则这个计数器为空。 12The Java Virtual Machine can support many threads of execution at once (JLS §17). Each Java Virtual Machine thread has its own pc (program counter) register. At any point, each Java Virtual Machine thread is executing the code of a single method, namely the current method (§2.6) for that thread. If that method is not native, the pc register contains the address of the Java Virtual Machine instruction currently being executed. If the method currently being executed by the thread is native, the value of the Java Virtual Machine&apos;s pc register is undefined. The Java Virtual Machine&apos;s pc register is wide enough to hold a returnAddress or a native pointer on the specific platform. 5.3.5Native Method Stacks(本地方法栈)如果当前线程执行的方法是Native类型的，这些方法就会在本地方法栈中执行。]]></content>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 基础梳理]]></title>
    <url>%2F2019%2F06%2F17%2Fpython-%E5%9F%BA%E7%A1%80%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习selenium笔记]]></title>
    <url>%2F2019%2F06%2F13%2Fpython%E4%BD%BF%E7%94%A8selenium%E5%81%9A%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[前言Selenium是一个浏览器自动化操作框架。可以模拟用户操作。这样我们就可以用selenium做很多事情了，测试自动化，爬虫。这里我接触和学习selenium也是使用来作为爬虫。 安装1pip install selenium 同时还需要在你的电脑上安装浏览器驱动。安装你本机浏览器对应版本的浏览器驱动。Chromedriver: http://npm.taobao.org/mirrors/chromedriver/ 具体操作控制浏览器操作的一些方法123456789101112set_window_size() 设置浏览器的大小back() 控制浏览器后退forward() 控制浏览器前进refresh() 刷新当前页面clear() 清除文本send_keys (value) 模拟按键输入click() 单击元素submit() 用于提交表单get_attribute(name) 获取元素属性值is_displayed() 设置该元素是否用户可见size 返回元素的尺寸text 获取元素的文本 鼠标事件12345678ActionChains(driver) 构造ActionChains对象context_click() 执行鼠标悬停操作move_to_element(above) 右击double_click() 双击drag_and_drop() 拖动move_to_element(above) 执行鼠标悬停操作context_click() 用于模拟鼠标右键操作， 在调用时需要指定元素定位perform() 执行所有 ActionChains 中存储的行为，可以理解成是对整个操作的提交动作 键盘事件123456789101112常用键盘操作send_keys(Keys.BACK_SPACE) 删除键（BackSpace）send_keys(Keys.SPACE) 空格键(Space)send_keys(Keys.TAB) 制表键(Tab)send_keys(Keys.ESCAPE) 回退键（Esc）send_keys(Keys.ENTER) 回车键（Enter）组合键盘操作send_keys(Keys.CONTROL,‘a’) 全选（Ctrl+A）send_keys(Keys.CONTROL,‘c’) 复制（Ctrl+C）send_keys(Keys.CONTROL,‘x’) 剪切（Ctrl+X）send_keys(Keys.CONTROL,‘v’) 粘贴（Ctrl+V）send_keys(Keys.F1…Fn) 键盘 F1…Fn 定位一组元素 定位一个元素 定位多个元素 含义 find_element_by_id find_elements_by_id 通过元素id定位 find_element_by_name find_elements_by_name 通过元素name定位 find_element_by_xpath find_elements_by_xpath 通过xpath表达式定位 find_element_by_link_text find_elements_by_link_tex 通过完整超链接定位 find_element_by_partial_link_text find_elements_by_partial_link_text 通过部分链接定位 find_element_by_tag_name find_elements_by_tag_name 通过标签定位 find_element_by_class_name find_elements_by_class_name 通过类名进行定位 find_elements_by_css_selector find_elements_by_css_selector 通过css选择器进行定位 示例代码页面html截图 12345678910111213141516171819202122from selenium import webdriverimport timebrowser = webdriver.Firefox()browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')time.sleep(5)liuyan_list = browser.find_elements_by_xpath('//*[@id="j-comment-section"]/div/div[3]/div/ul[3]/li')#获取用户留言liuyan_content = liuyan_list[0].find_element_by_class_name('shiyongxinde.oh')print(liuyan_content.text)liuyan_shijian = liuyan_list[0].find_element_by_class_name('info-time').find_element_by_tag_name('a').text#获取留言时间print(liuyan_shijian)liuyan_id = liuyan_list[0].find_element_by_class_name('woZanTong').get_attribute('appraiseid')#获取留言idprint(liuyan_id)nick_name = liuyan_list[0].find_element_by_class_name('reply_avatar_userName').text#获取昵称print(nick_name) 多窗口切换在页面操作过程中有时候点击某个链接会弹出新的窗口，这时就需要主机切换到新打开的窗口上进行操作。WebDriver提供了switch_to.window()方法，可以实现在不同的窗口之间切换。 1234current_window_handle 获得当前窗口句柄window_handles 返回所有窗口的句柄到当前会话switch_to.window() 用于切换到相应的窗口，与上一节的switch_to.frame()类似，前者用于不同窗口的切换，后者用于不同表单之间的切换。 调用JavaScript代码虽然WebDriver提供了操作浏览器的前进和后退方法，但对于浏览器滚动条并没有提供相应的操作方法。在这种情况下，就可以借助JavaScript来控制浏览器的滚动条。WebDriver提供了execute_script()方法来执行JavaScript代码。 12直接翻到当前页面的页尾browser.execute_script('window.scrollTo(0, document.body.scrollHeight)') cookie操作12345get_cookies() 获得所有cookie信息get_cookie(name) 返回字典的key为“name”的cookie信息add_cookie(cookie_dict) 添加cookie。“cookie_dict”指字典对象，必须有name 和value 值delete_cookie(name,optionsString) 删除cookie信息。“name”是要删除的cookie的名称，“optionsString”是该cookie的选项，目前支持的选项包括“路径”，“域”delete_all_cookies() 删除所有cookie信息 关闭浏览器123close() 关闭单个窗口quit() 关闭所有窗口 可能遇到的坑页面加载慢因为sulenium是代码控制浏览器操作，代码执行很快，浏览器执行很慢，经常发生代码执行到这里，但是页面资源没有加载完的情况。所以需要 time.sleep(5) 让程序等待几秒钟。 莫名其妙的异常我代码写的很开心，执行一下突然报错。。。就是那种明明上一次启动还正常，可能写了一行注释回来执行就报错了。。。让我不知所措，最后是restart大法好。重启就好了，我没有找到发生的具体原因，目前是出现就重启。。。 123456789/Users/luzhengxiang/anaconda3/envs/my_spider/bin/python /Users/luzhengxiang/PycharmProjects/autoSign/spiders/selenium_test/gome/guomei_selenium.pyTraceback (most recent call last): File "/Users/luzhengxiang/PycharmProjects/autoSign/spiders/selenium_test/gome/guomei_selenium.py", line 10, in &lt;module&gt; browser = webdriver.Chrome(); File "/Users/luzhengxiang/anaconda3/envs/my_spider/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py", line 73, in __init__ self.service.start() File "/Users/luzhengxiang/anaconda3/envs/my_spider/lib/python3.7/site-packages/selenium/webdriver/common/service.py", line 104, in start raise WebDriverException("Can not connect to the Service %s" % self.path)selenium.common.exceptions.WebDriverException: Message: Can not connect to the Service chromedriver 不同浏览器内核可能支持的方法不同我最开始基于firefox写的代码，后面直接替换成了chrome。很多原本正常的方法就报错了。猜测原因就是可能不同浏览器对这个selenium的支持不一样导致的。 参考链接：https://blog.csdn.net/weixin_36279318/article/details/79475388]]></content>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 常用命令]]></title>
    <url>%2F2019%2F06%2F12%2Flinux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[工作中常用的命令记录查询12345678910111213141516171819202122lsof -i:端口号 查看端口被什么进程占用netstat -tunlp 查看端口使用情况。ps -ef|grep xxx 查询xxx的进程使用情况文件中根据关键字搜索Cat filename | grep XXX |grep XXX例如：cat zhang8online3.0.log |grep 50641658625520文件中根据关键字统计行数场景：查询接口调用次数cat localhost_access_log.2018-06-06.txt |grep getverifycode |wc -l例如：cat localhost_access_log.2018-06-07.txt |grep getverifycode |wc -l查询中使用正则匹配场景：查询耗时长的接口。cat zhang8online3.0.log | grep -E "耗时：[0-9]&#123;4,&#125;"df -h 查询 磁盘使用情况 操作命令12345678910111213往文件中写值，覆盖文件内容。在不删除文件的情况下清空文件内容echo "" &gt; ZH8.online.trade.logchmod 777 文件名 文件给权限解压tar包tar -xvf file.tar //解压 tar包tar -xzvf file.tar.gz //解压tar.gztar -xjvf file.tar.bz2 //解压 tar.bz2tar -xZvf file.tar.Z //解压tar.Zunrar e file.rar //解压rarunzip file.zip //解压zip 定时任务1234crontab -helpcrontab -l 查看系统定时任务]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F06%2F11%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[热烈庆祝拖延症晚期的陆大大终于搭建了自己的个人博客啦看到很多人都有自己的博客，记录生活中的点点滴滴，记录自己的喜怒哀乐，记录学习的新技术，记录工作中踩的坑。弄一个自己的博客玩玩的想法在我心里其实很久了，但是直到今天，都2019年了，我才真的肯花点心思来弄，我的这个拖延症呀，我是真的都佩服我自己。不过有句话怎么说的呢，觉得为时已晚的时候，恰恰是是最早的时候。。。。 自我介绍一下我是90狮子座的沪漂程序猿，喜欢唱，跳，篮球……不对不对不对，跑偏了，哈哈哈。我的主要技术方向是java，但是现在公司业务需要整合了几条业务线，我负责的项目都交接给了别的同事，现在的我挺迷茫的，公司的安排是我们组作为后背力量，准备应对可能出现的其它项目，然后整理公司的数据。参与数据的开发，（个人感觉我真的是在在失业的边缘徘徊。。。）所以我现在是什么都会去涉及一点，nodejs，python，elk，因为接触时间补偿，目前都涉入不深。 2012-9月份 - 2019年6月我还记得我是2012年的9月份来的上海，到现在已经7年了，我的妈呀，时间是真的可怕。我一点也不想承认我已经26岁，已经在脱发和被家人逼婚的路上越走越走越远。每天上班下班的日子其实说不上辛苦，但是真的觉得少了很多激情，我也已经不像以前刚工作那会，有干劲到可以连续上班一个月，天天11，2点下班还乐此不疲，觉得这就是奋斗，这是年轻人应该过的生活。人总是想着自己没有的，然后容易忽略掉自己拥有的东西，很显然我就是这一类人的代表了。总是很容易患得患失。其实都是给作的。 最后当然是总结陈词啦虽然自己挺多不好，但是还是不影响我是一个很阳光的宝藏男孩啦。以后我就要多写文章和技术博客来记录我的学习历程，和知识难点。好记性不如烂博客。然后就是多写文章来记录生活重的点点滴滴啦。]]></content>
      <tags>
        <tag>心情杂记</tag>
      </tags>
  </entry>
</search>
